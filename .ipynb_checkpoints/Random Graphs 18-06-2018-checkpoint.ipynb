{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## CONTENTS\n",
    "\n",
    "# (1) scatter/heat map for variables by pan-dataset values\n",
    "# (2) What's the balance between for/against Single Market?\n",
    "# (3) Is there a pronounced ethnic (White British/Not) split on Brexit(happyEULeave)?\n",
    "# (4) likeCorbyn by average static values over time\n",
    "# (5) Covariation in al lr scales variables which creates the weird distributions\n",
    "# (6) Who are the centrists?\n",
    "# (7) Relative political positioning by *where you place parties*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import gc\n",
    "import re\n",
    "\n",
    "from itertools import cycle\n",
    "from IPython.display import display\n",
    "import pickle, os\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "encoding = \"ISO-8859-1\"\n",
    "\n",
    "import Jupyter_module_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you should clone this git to this subdirectory (in some directory - I call it BES_analysis - doesn't matter though)\n",
    "\n",
    "if os.getcwd().split(os.sep)[-1] != 'BES_analysis_code':\n",
    "    raise Exception(\"Stop! You're in the wrong directory - should be in 'BES_analysis_code'\")\n",
    "\n",
    "BES_code_folder   = \"../BES_analysis_code/\" # we should be here!\n",
    "BES_small_data_files = BES_code_folder + \"small data files\" + os.sep\n",
    "if not os.path.exists( BES_small_data_files ):\n",
    "    os.makedirs( BES_small_data_files )\n",
    "\n",
    "# we should create these if they don't already exist\n",
    "BES_data_folder   = \"../BES_analysis_data/\"\n",
    "if not os.path.exists( BES_data_folder ):\n",
    "    os.makedirs( BES_data_folder )\n",
    "\n",
    "BES_output_folder = \"../BES_analysis_output/\"\n",
    "if not os.path.exists( BES_output_folder ):\n",
    "    os.makedirs( BES_output_folder )\n",
    "    \n",
    "BES_file_manifest = pd.read_csv( BES_small_data_files + \"BES_file_manifest.csv\" )\n",
    "\n",
    "BES_R_data_files = BES_data_folder + \"R_data\" + os.sep\n",
    "if not os.path.exists( BES_R_data_files ):\n",
    "    os.makedirs( BES_R_data_files )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"W13_comb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "manifest = BES_file_manifest[ BES_file_manifest[\"Name\"] == dataset_name ]\n",
    "\n",
    "data_subfolder = BES_data_folder + dataset_name + os.sep\n",
    "\n",
    "dataset_filename = manifest[\"Stata_Filename\"].values[0]\n",
    "# dataset_description = manifest[\"Friendlier_Description\"].values[0]\n",
    "# dataset_citation = manifest[\"Citation\"].values[0]\n",
    "# dataset_start = manifest[\"Date_Start\"].values[0]\n",
    "# dataset_stop = manifest[\"Date_Stop\"].values[0]\n",
    "# dataset_wave = manifest[\"Wave No\"].values[0]\n",
    "\n",
    "BES_Panel = pd.read_stata( data_subfolder + dataset_filename )\n",
    "print(\"BES_Panel\", BES_Panel.shape )\n",
    "\n",
    "####\n",
    "\n",
    "BES_numeric = pd.read_hdf( data_subfolder + \"BESnumeric.hdf\", \"BESnumeric\" )\n",
    "print(\"BES_numeric\",  BES_numeric.shape )\n",
    "\n",
    "var_type    = pd.read_csv( data_subfolder + \"var_type.csv\", encoding=encoding)\n",
    "var_type.set_index(\"Unnamed: 0\", inplace=True)\n",
    "print(\"var_type\",  var_type.shape )\n",
    "\n",
    "fname = data_subfolder + \"cat_dictionary.pkl\"\n",
    "with open(fname, \"rb\") as f:\n",
    "    cat_dictionary = pickle.load( f )\n",
    "    \n",
    "####\n",
    "\n",
    "BES_non_numeric = pd.read_hdf( data_subfolder + \"BESnon_numeric.hdf\", \"BESnon_numeric\" )\n",
    "print(\"BES_non_numeric\",  BES_non_numeric.shape )\n",
    "\n",
    "BES_reduced = pd.read_hdf( data_subfolder + \"BES_reduced.hdf\", \"BES_reduced\" )\n",
    "print(\"BES_reduced\",  BES_reduced.shape )\n",
    "\n",
    "BES_reduced_with_na = pd.read_hdf( data_subfolder + \"BES_reduced_with_na.hdf\", \"BES_reduced_with_na\")\n",
    "print(\"BES_reduced_with_na\", BES_reduced_with_na.shape )\n",
    "\n",
    "fname = data_subfolder + \"new_old_col_names.pkl\"\n",
    "with open(fname, \"rb\") as f:\n",
    "    new_old_col_names = pickle.load(f) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.decomposition import PCA, IncrementalPCA, NMF, TruncatedSVD, FastICA, FactorAnalysis, SparsePCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "from gaussian_kde import gaussian_kde\n",
    "from utility import display_components, display_pca_data, weighted_kde"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (1) scatter/heat map for variables by pan-dataset values ####\n",
    "\n",
    "# Initial motivation: Where does Corbyn's appeal to the Left outweigh his repulsion to Authoritarians?\n",
    "# output: likeCorbyn_byvalues_overtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pan_dataset_values = pd.read_csv( BES_small_data_files + \"pan_dataset_values.csv\", encoding=encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recreate granularity at whatever level\n",
    "\n",
    "granularity = 40\n",
    "pan_dataset_values[\"lr_uniform\"] = pd.qcut( pan_dataset_values[\"raw_gen_lr_vals\"], q=granularity, labels=range(0,granularity))\n",
    "pan_dataset_values[\"al_uniform\"] = pd.qcut( pan_dataset_values[\"raw_gen_al_vals\"], q=granularity, labels=range(0,granularity))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check granularity\n",
    "pan_dataset_values[\"al_uniform\"].hist(bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at interesting like variables\n",
    "[(x,BES_Panel[x].notnull().sum()) for x in sorted(BES_Panel.columns) if \"like\" in x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(like_var, like_name) = (\"likeMay\", \"Theresa May\")\n",
    "(like_var, like_name) = (\"likeBNP\", \"British National Party\")\n",
    "(like_var, like_name) = (\"likeBennett\", \"Natalie Bennett\")\n",
    "(like_var, like_name) = (\"likeBlair\", \"Tony Blair\")\n",
    "(like_var, like_name) = (\"likeBoris\", \"Boris Johnson\")\n",
    "(like_var, like_name) = (\"likeCameron\", \"David Cameron\")\n",
    "(like_var, like_name) = (\"likeClegg\", \"Nick Clegg\")\n",
    "(like_var, like_name) = (\"likeCon\", \"The Conservative Party\")\n",
    "(like_var, like_name) = (\"likeDavis\", \"David Davis\")\n",
    "(like_var, like_name) = (\"likeFarage\", \"Nigel Farage\")\n",
    "(like_var, like_name) = (\"likeFarron\", \"Tim Farron\")\n",
    "(like_var, like_name) = (\"likeJarvis\", \"Dan Jarvis\")\n",
    "(like_var, like_name) = (\"likeLD\", \"The Liberal Democrats\")\n",
    "(like_var, like_name) = (\"likeLab\", \"The Labour Party\")\n",
    "(like_var, like_name) = (\"likeLucas\", \"Caroline Lucas\")\n",
    "(like_var, like_name) = (\"likeMcDonnell\", \"John McDonnell\")\n",
    "(like_var, like_name) = (\"likeMiliband\", \"Ed Miliband\")\n",
    "(like_var, like_name) = (\"likeNuttall\", \"Paul Nuttall\")\n",
    "(like_var, like_name) = (\"likeOsborne\", \"George Osborne\")\n",
    "# (like_var, like_name) = (\"likePC\", \"Plaid Cymru\")\n",
    "(like_var, like_name) = (\"likeRudd\", \"Amber Rudd\")\n",
    "(like_var, like_name) = (\"likeSEvans\", \"Suzanne Evans\")\n",
    "(like_var, like_name) = (\"likeSNP\", \"The Scottish Nationalist Party\")\n",
    "# (like_var, like_name) = (\"likeSalmond\", \"Alex Salmond\")\n",
    "(like_var, like_name) = (\"likeSturgeon\", \"Nicola Sturgeon\")\n",
    "(like_var, like_name) = (\"likeUKIP\", \"The UK Independence Party\")\n",
    "(like_var, like_name) = (\"likeWatson\", \"Tom Watson\")\n",
    "(like_var, like_name) = (\"likeWood\", \"Leanne Wood\")\n",
    "#\n",
    "(like_var, like_name) = (\"euPriorityBalance\", \"euPriorityBalance\")\n",
    "(like_var, like_name) = (\"immigSelf\", \"immigSelf\")\n",
    "(like_var, like_name) = (\"likeCorbyn\", \"Jeremy Corbyn\")\n",
    "(like_var, like_name) = (\"likeMay\", \"Theresa May\")\n",
    "\n",
    "x_axis = \"al_uniform\"\n",
    "y_axis = \"lr_uniform\"\n",
    "\n",
    "\n",
    "mean_like_Corbyn = BES_Panel[[x for x in BES_Panel.columns if like_var in x]]\\\n",
    ".replace(\"Don't know\",np.nan).apply(lambda x:x.cat.codes).replace(-1,np.nan).mean(axis=1)\n",
    "mask = mean_like_Corbyn.notnull()\n",
    "\n",
    "likeCorbyn_heatmap = pd.crosstab(index = pan_dataset_values[y_axis][mask],\n",
    "            columns = pan_dataset_values[x_axis][mask],\n",
    "            values = mean_like_Corbyn[mask],\n",
    "            aggfunc = np.mean)\n",
    "\n",
    "plt.figure(figsize = (16,10))\n",
    "ax = sns.heatmap(data = likeCorbyn_heatmap.replace(np.nan,-1),\n",
    "            cbar_kws={'label': like_var+ ' 0-Strongly dislike, 10-Strongly like'})\n",
    "ax.invert_yaxis()\n",
    "# plt.colorbar(points);\n",
    "plt.xlabel(\"libertarian-authoritarian (flattened)\");\n",
    "plt.ylabel(\"economic_right-economic_left (flattened)\");\n",
    "plt.title(\"How much do you like or dislike \"+like_name+\"?\\n(mean value, -1 = no data))\\n(N = \"+str(mask.sum())+\")\")\n",
    "plt.savefig(BES_output_folder + \"likeCorbyn_byvalues_overtime\" + os.sep + like_var +\"_by_allr_heatmap\" + \".png\")\n",
    "\n",
    "mean_like_Corbyn = BES_Panel[[x for x in BES_Panel.columns if like_var in x]]\\\n",
    ".replace(\"Don't know\",np.nan).apply(lambda x:x.cat.codes).replace(-1,np.nan).mean(axis=1)\n",
    "mask = mean_like_Corbyn.notnull()\n",
    "\n",
    "x_axis = \"xt_std_al\"\n",
    "y_axis = \"xt_std_lr\"\n",
    "\n",
    "corbynLike_by_values = pan_dataset_values[[y_axis,x_axis]][mask]\n",
    "corbynLike_by_values[like_var] = mean_like_Corbyn[mask]\n",
    "\n",
    "plt.figure(figsize = (16,10))\n",
    "points = plt.scatter(data = corbynLike_by_values, x = x_axis, y = y_axis, c = like_var,\n",
    "                     cmap=\"brg\", alpha=.5, s=10)\n",
    "cbar = plt.colorbar(points);\n",
    "cbar.set_label(like_var+' 0-Strongly dislike, 10-Strongly like', rotation=270)\n",
    "plt.xlabel(\"libertarian-authoritarian (normalised)\");\n",
    "plt.ylabel(\"economic_right-economic_left (normalised)\");\n",
    "plt.title(\"How much do you like or dislike \"+like_name+\"?\\n(N = \"+str(mask.sum())+\")\")\n",
    "plt.savefig(BES_output_folder + \"likeCorbyn_byvalues_overtime\" + os.sep + like_name+\"_by_allr_scatter\" + \".png\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (2) What's the balance between for/against Single Market?\n",
    "# pretty much exactly 50:50 as of post GE2017\n",
    "\n",
    "# output folder \"randomFunPlots\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "euprior = BES_Panel['euPriorityBalanceW13'].replace(\"Don't know\", np.nan).cat.codes.replace(-1,np.nan)\n",
    "mask = euprior.notnull()\n",
    "weighted_mean = (euprior[mask] * BES_Panel['wt_new_W13'][mask]).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "euprior[mask].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,6))\n",
    "sns.countplot(BES_Panel['euPriorityBalanceW13']);\n",
    "plt.xticks(rotation=90);\n",
    "plt.title(\"Brexit priority: access to single market versus controlling immigration?\\n(BES Wave 13, N = \"+str(mask.sum()) +\")\\nDemographically weighted mean (-DKs) = \"+str(round(weighted_mean,2)));\n",
    "dataset_citation = \"Source: \" + manifest[\"Citation\"].values[0]\n",
    "plt.annotate(dataset_citation, (0,0), (0, -165),\n",
    "                             xycoords='axes fraction', textcoords='offset points', va='top', fontsize = 7)  \n",
    "plt.savefig(BES_output_folder + \"randomFunPlots\" + os.sep + \"single_market_vs_controlling_immigration\" + \".png\",bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (3) Is there a pronounced ethnic (White British/Not) split on Brexit(happyEULeave)?\n",
    "# not really (general recollection: the driver isn't ethnicity but differences in values)\n",
    "# coda: p_surridge says BES ethnic variable not very good\n",
    "\n",
    "# output folder: \"randomFunPlots\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Leavers = BES_Panel[\"profile_eurefvote\"] == \"Leave the EU\"\n",
    "Remainers = BES_Panel[\"profile_eurefvote\"] == \"Stay/remain in the EU\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OneDropWhiteBritish = (BES_Panel[[x for x in BES_Panel.columns if \"ethnicity\" in x]]==\"White British\").any(axis=1)\n",
    "OneDropWhiteBritish = BES_Panel[\"profile_ethnicity\"]==\"White British\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = (Remainers) & (OneDropWhiteBritish) & BES_Panel['happyEULeaveW13'].notnull()\n",
    "\n",
    "\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.subplot(111)\n",
    "sns.countplot(BES_Panel['happyEULeaveW13'][mask]);\n",
    "plt.xticks(rotation=90);\n",
    "plt.title(\"White British Remainers: How happy or how disappointed are you that the UK voted to leave the EU?\\n(BES Wave 13, N = \"+str(mask.sum()) +\")\");\n",
    "dataset_citation = \"Source: \" + manifest[\"Citation\"].values[0]\n",
    "plt.annotate(dataset_citation, (0,0), (0, -165),\n",
    "                             xycoords='axes fraction', textcoords='offset points', va='top', fontsize = 7)  \n",
    "plt.savefig(BES_output_folder + \"randomFunPlots\" + os.sep + \"happyEULeave_Remainers_by_ethnicity_WB\" + \".png\",\n",
    "            bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (4) likeCorbyn by average static values over time\n",
    "# also include White British & Not London subsample\n",
    "\n",
    "\n",
    "# output folder: \"likeCorbyn_byvalues_overtime\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pan_dataset_values = pd.read_csv( BES_small_data_files + \"pan_dataset_values.csv\", encoding=encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "likeCorbyn_panvalues_corr = pd.concat([ BES_reduced[[x for x in BES_reduced if re.match(\"likeCorbyn\",x)]],\n",
    "                                        pan_dataset_values[[\"xt_std_lr\",\"xt_std_al\"]]],axis=1).corr()\n",
    "\n",
    "Corbyn_over_time = likeCorbyn_panvalues_corr[[\"xt_std_lr\",\"xt_std_al\"]].drop([\"xt_std_lr\",\"xt_std_al\"])\n",
    "\n",
    "Corbyn_over_time_sns = Corbyn_over_time.copy()\n",
    "Corbyn_over_time_sns[\"xt_std_al\"] = -Corbyn_over_time_sns[\"xt_std_al\"]\n",
    "Corbyn_over_time_sns = Corbyn_over_time_sns.stack().reset_index()\n",
    "Corbyn_over_time_sns.columns = [\"likeCorbyn\", \"value scale\",\"correlation\"]\n",
    "Corbyn_over_time_sns[\"value scale\"] = Corbyn_over_time_sns[\"value scale\"].replace(\"xt_std_lr\",\"economic(right_left)\").replace(\"xt_std_al\",\"social(conservative_liberal)\")\n",
    "Corbyn_over_time_sns[\"likeCorbyn\"] = [re.search(\"(W\\d+)\",x).group(0) for x in Corbyn_over_time_sns[\"likeCorbyn\"]]\n",
    "g = sns.pointplot(data = Corbyn_over_time_sns, x = \"likeCorbyn\", hue = \"value scale\", y=\"correlation\");\n",
    "# plt.xticks(rotation=45);\n",
    "plt.savefig(BES_output_folder +os.sep+ \"likeCorbyn_byvalues_overtime\" + os.sep + \"likeCorbyn_byvalues_overtime\" + \".png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "values = [\"xt_std_lr\",\"xt_std_al\",\"genImmigSentiment\"]\n",
    "likeCorbyn_panvalues_corr = pd.concat([ BES_reduced[[x for x in BES_reduced if re.match(\"likeCorbyn\",x)]],\n",
    "                                        pan_dataset_values[values]],axis=1).corr()\n",
    "\n",
    "Corbyn_over_time = likeCorbyn_panvalues_corr[values].drop(values)\n",
    "\n",
    "Corbyn_over_time_sns = Corbyn_over_time.copy()\n",
    "Corbyn_over_time_sns[\"xt_std_al\"] = -Corbyn_over_time_sns[\"xt_std_al\"]\n",
    "Corbyn_over_time_sns = Corbyn_over_time_sns.stack().reset_index()\n",
    "Corbyn_over_time_sns.columns = [\"likeCorbyn\", \"value scale\",\"correlation\"]\n",
    "Corbyn_over_time_sns[\"value scale\"] = Corbyn_over_time_sns[\"value scale\"]\\\n",
    "    .replace(\"xt_std_lr\",\"economic(right_left)\")\\\n",
    "    .replace(\"xt_std_al\",\"social(conservative_liberal)\")\\\n",
    "    .replace(\"genImmigSentiment\",\"immigration(negative_positive)\")\n",
    "Corbyn_over_time_sns[\"likeCorbyn\"] = [re.search(\"(W\\d+)\",x).group(0) for x in Corbyn_over_time_sns[\"likeCorbyn\"]]\n",
    "\n",
    "wave_dates = BES_file_manifest[ BES_file_manifest[\"Only_or_Combined\"] == \"Only\" ][[\"Wave No\",\"Date_Start\"]]\n",
    "wave_dates[\"Wave No\"] = wave_dates[\"Wave No\"].apply(lambda x: \"W\"+str(x))\n",
    "wave_dates = wave_dates.set_index(\"Wave No\")\n",
    "Corbyn_over_time_sns[\"likeCorbyn\"] = Corbyn_over_time_sns[\"likeCorbyn\"].apply(lambda x: wave_dates.loc[x].values[0])\n",
    "\n",
    "plt.figure(figsize = (10,6))\n",
    "\n",
    "g = sns.pointplot(data = Corbyn_over_time_sns, x = \"likeCorbyn\", hue = \"value scale\", y=\"correlation\");\n",
    "# plt.xticks(rotation=45);\n",
    "\n",
    "# likeCorbyn_byvalues_overtime_includingImmig\n",
    "\n",
    "plt.xlabel(\"Wave Date\")\n",
    "plt.title(\"How much do you like or dislike Jeremy Corbyn?\")\n",
    "plt.savefig(BES_output_folder +os.sep+ \"likeCorbyn_byvalues_overtime\" + os.sep + \"likeCorbyn_byvalues_overtime_clearer\" + \".png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pan_dataset_values[\"female\"] = pd.get_dummies(pan_dataset_values[\"gender\"], drop_first=True)[\"Female\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DefinitelyNotLondon = (BES_Panel[\"gor\"]!=\"London\") & (BES_Panel[\"gor\"].notnull())\n",
    "DefinitelyWhiteBritish = (BES_Panel[\"profile_ethnicity\"]==\"White British\") & (BES_Panel[\"profile_ethnicity\"].notnull())\n",
    "mask = DefinitelyWhiteBritish & DefinitelyNotLondon\n",
    "\n",
    "mask.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (10,6))\n",
    "\n",
    "g = sns.pointplot(data = Corbyn_over_time_sns, x = \"likeCorbyn\", hue = \"value scale\", y=\"correlation\");\n",
    "\n",
    "\n",
    "values = [\"xt_std_lr\",\"xt_std_al\",\"genImmigSentiment\"]\n",
    "likeCorbyn_panvalues_corr = pd.concat([ BES_reduced[[x for x in BES_reduced if re.match(\"likeCorbyn\",x)]],\n",
    "                                        pan_dataset_values[values]],axis=1)[mask].corr()\n",
    "\n",
    "Corbyn_over_time = likeCorbyn_panvalues_corr[values].drop(values)\n",
    "\n",
    "label = \"(WB&NL)\"\n",
    "\n",
    "Corbyn_over_time_sns = Corbyn_over_time.copy()\n",
    "Corbyn_over_time_sns[\"xt_std_al\"] = -Corbyn_over_time_sns[\"xt_std_al\"]\n",
    "Corbyn_over_time_sns = Corbyn_over_time_sns.stack().reset_index()\n",
    "Corbyn_over_time_sns.columns = [\"likeCorbyn\", \"value scale\",\"correlation\"]\n",
    "Corbyn_over_time_sns[\"value scale\"] = Corbyn_over_time_sns[\"value scale\"]\\\n",
    "    .replace(\"xt_std_lr\",\"economic(right_left)\"+label)\\\n",
    "    .replace(\"xt_std_al\",\"social(conservative_liberal)\"+label)\\\n",
    "    .replace(\"genImmigSentiment\",\"immigration(negative_positive)\"+label)\n",
    "Corbyn_over_time_sns[\"likeCorbyn\"] = [re.search(\"(W\\d+)\",x).group(0) for x in Corbyn_over_time_sns[\"likeCorbyn\"]]\n",
    "\n",
    "wave_dates = BES_file_manifest[ BES_file_manifest[\"Only_or_Combined\"] == \"Only\" ][[\"Wave No\",\"Date_Start\"]]\n",
    "wave_dates[\"Wave No\"] = wave_dates[\"Wave No\"].apply(lambda x: \"W\"+str(x))\n",
    "wave_dates = wave_dates.set_index(\"Wave No\")\n",
    "Corbyn_over_time_sns[\"likeCorbyn\"] = Corbyn_over_time_sns[\"likeCorbyn\"].apply(lambda x: wave_dates.loc[x].values[0])\n",
    "# Corbyn_over_time_sns[\"gender\"] = BES_Panel[\"gender\"]\n",
    "\n",
    "# plt.figure(figsize = (10,6))\n",
    "\n",
    "sns.pointplot(data = Corbyn_over_time_sns, x = \"likeCorbyn\", hue = \"value scale\", y=\"correlation\", linestyles=\"--\", markers = \"v\");\n",
    "# plt.xticks(rotation=45);\n",
    "# likeCorbyn_byvalues_overtime_includingImmig\n",
    "\n",
    "plt.title(\"likeCorbyn_WhiteBritish_and_NotLondon (N = \"+str(mask.sum()) +\")\");\n",
    "plt.savefig(BES_output_folder +os.sep+ \"likeCorbyn_byvalues_overtime\" + os.sep + \"likeCorbyn_WhiteBritish_and_NotLondon_comparable\" + \".png\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (5) Covariation in al lr scales variables which creates the weird distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10,10))\n",
    "ax = plt.gca()\n",
    "BES_vals[[x for x in BES_Panel.columns if re.match(\"(al|lr)\\dW1W2W3W4W5\",x)]].hist(ax=ax, bins=5);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10,10))\n",
    "ax = plt.gca()\n",
    "al_lr_vals = [x for x in BES_Panel.columns if re.match(\"(al|lr)\\dW13\",x)]\n",
    "BES_vals = BES_Panel[al_lr_vals]\n",
    "BES_vals = BES_vals.replace(\"Don't know\",np.nan).dropna().apply(lambda x: x.cat.codes)\n",
    "BES_vals[al_lr_vals].hist(ax=ax, bins=5);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_vals = [x for x in BES_Panel.columns if re.match(\"(lr)\\dW13\",x)]\n",
    "lr_corr = pd.get_dummies(BES_Panel[lr_vals].replace(\"Don't know\",np.nan).dropna()).corr()\n",
    "lr_corr = lr_corr.drop([x for x in lr_corr.index if \"Don't know\" in x]).drop([x for x in lr_corr.columns if \"Don't know\" in x],axis=1)\n",
    "lr_corr = lr_corr.where(np.triu(np.ones(lr_corr.shape), k=5).astype(np.bool))\n",
    "lr_corr.stack().sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_corr.stack().sort_values().head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_corr.stack().sort_values().tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (6) Who are the centrists?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Centrist_corr = BES_reduced.corrwith( (BES_Panel[[x for x in BES_Panel if \"leftRight\" in x]]==\"5\").mean(axis=1) )\n",
    "Centrist_corr.sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "everCentrist = np.any(BES_Panel[[x for x in BES_Panel if \"leftRight\" in x]]==\"5\",axis=1).astype('float')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "everCentrist_corr = BES_reduced.corrwith( pd.DataFrame(everCentrist)[0] )\n",
    "everCentrist_corr.sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "everCentrist_corr.loc[[x for x in BES_reduced.columns if \"lr_scale\" in x]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "everCentrist_corr.loc[[x for x in BES_reduced.columns if \"al_scale\" in x]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (7) Relative political positioning by *where you place parties*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_placement = [x for x in BES_reduced if re.match(\"lr[^0-9_]+W\\d+__Right\"+\\\n",
    "                                    '|EUIntegration[^S]+W\\d+__Protect our independence'\\\n",
    "                                    '|immig[^S]+W\\d+__Allow many more'\\\n",
    "                                    '|tryReduceImmig\\w+W\\d+_Yes'\\\n",
    "                                    '|tryReduceInequality\\w+W\\d+_Yes'\\\n",
    "                                    '|certaintyEU\\w+W\\d+__Very certain'\\\n",
    "                                    '|conPartner\\w+W\\d+_Would join Conservatives'\\\n",
    "                                    '|labPartner\\w+W\\d+_Would join Labour'\\\n",
    "                                    '|noChanceCoalition\\w+W\\d+_Yes'\\\n",
    "                                    ,x)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Factor Analysis by <other>placement\n",
    "\n",
    "# sorted([x for x in BES_reduced if \"UKIP\" in x])\n",
    "# Placement\n",
    "# 'lrUKIPW2__Right'\n",
    "# 'EUIntegrationUKIPW7__Protect our independence'\n",
    "# 'immigUKIPW7__Allow many more'\n",
    "# 'tryReduceImmigUKIPW4_Yes',\n",
    "# 'tryReduceInequalityUKIPW4_Yes',\n",
    "# 'certaintyEUUKIPW8__Very certain'\n",
    "# 'conPartnerUKIPW4_Would join Conservatives'\n",
    "# 'labPartnerUKIPW4_Would join Labour'\n",
    "# 'noChanceCoalitionUKIPW11_Yes'\n",
    "\n",
    "# Preference\n",
    "# 'winConstituencyUKIPW4__100 - Very likely to win'\n",
    "#'handleMIIUKIPW5__Very well'\n",
    "#'likeUKIPW7__Strongly like'\n",
    "#'ptvUKIPW7__Very likely'\n",
    "#'achieveReduceImmigUKIPW11_Yes' ?\n",
    "# 'ashcroftW4_United Kingdom Independence Party (UKIP)'\n",
    "#'bestOnMIIW11_United Kingdom Independence Party (UKIP)'\n",
    "\n",
    "#'generalElectionVoteW10_United Kingdom Independence Party (UKIP)'\n",
    "# 'partyIdW10_United Kingdom Independence Party (UKIP)'\n",
    "#'profile_past_vote_2005_United Kingdom Independence Party (UKIP)'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " [x for x in BES_reduced if re.match(\"lr[^_]+W\\d+(__Right|__Strongly agree)|leftRightW\\d+__Right\",x)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lr placement inc self + lr values\n",
    "lr_placement_inc_self_lr_vals = [x for x in BES_reduced if re.match(\"lr[^_]+W\\d+(__Right|__Strongly agree)|leftRightW\\d+__Right\",x)]\n",
    "BES = BES_reduced[lr_placement_inc_self_lr_vals]\n",
    "Treatment = dataset_name + \"lr_placement_inc_self_lr_Vals\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lr placement inc self\n",
    "lr_placement_inc_self = [x for x in BES_reduced if re.match(\"lr[^0-9_]+W\\d+__Right|leftRightW\\d+__Right\",x)]\n",
    "BES = BES_reduced[lr_placement_inc_self]\n",
    "Treatment = dataset_name + \"lr_placement_inc_self\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BES = BES_reduced[lr_placement]\n",
    "Treatment = dataset_name + \"lr_placement\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#all_placement\n",
    "BES = BES_reduced[all_placement]\n",
    "Treatment = dataset_name + \"all_placement\"\n",
    "# confusing!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_subfolder = BES_output_folder + Treatment + os.sep\n",
    "if not os.path.exists( output_subfolder ):\n",
    "    os.makedirs( output_subfolder )\n",
    "\n",
    "# normalise data (subtract out the mean, divide through by standard deviation)\n",
    "\n",
    "clean_feature_set_std = StandardScaler().fit_transform(BES.values )\n",
    "BES_std = pd.DataFrame(      clean_feature_set_std,\n",
    "                             columns = BES.columns,\n",
    "                             index   = BES.index      )\n",
    "    \n",
    "\n",
    "n_components = BES_std.shape[1]\n",
    "# n_components = 3\n",
    "decomp = FactorAnalysis(svd_method = 'lapack',n_components = n_components) ## ~10s ,n_components=30 -> 1.5 hrs\n",
    "decomp_method = str(decomp).split(\"(\")[0] \n",
    "\n",
    "X_r = decomp.fit_transform(BES_std)\n",
    "\n",
    "BES_decomp = pd.DataFrame(   X_r,\n",
    "                             columns = range(0,n_components),\n",
    "                             index   = BES_std.index)\n",
    "                             \n",
    "\n",
    "load_suff = \"FactorAnalysis\"\n",
    "save = True # False => Load\n",
    "\n",
    "if save & ( 'decomp' in globals() ): # SAVE    ##( 'decomp' not in globals() )\n",
    "    decomp_method = str(decomp).split(\"(\")[0] \n",
    "    subdir = output_subfolder + decomp_method\n",
    "    fname = subdir+ os.sep + decomp_method\n",
    "    # create dir, save decomp object, BES_decomp, BES_std    \n",
    "    if not os.path.exists(subdir): os.makedirs(subdir)\n",
    "    with open(fname+\".pkl\", \"wb\") as f: pickle.dump( decomp, f )\n",
    "    BES_decomp.to_hdf(fname+\".hdf\"        , decomp_method)\n",
    "    BES_std.to_hdf(   fname+\"_std\"+\".hdf\" , decomp_method)\n",
    "    \n",
    "else: # LOAD decomp results (default is SAVE)\n",
    "    decomp_method = load_suff\n",
    "    subdir = output_subfolder + os.sep + decomp_method    \n",
    "    fname = subdir + os.sep + decomp_method\n",
    "    if not os.path.exists(subdir): raise Exception(subdir + ' does not exist!')\n",
    "    # load decomp object, BES_decomp, BES_std, n_components\n",
    "    with open(fname+\".pkl\", \"rb\") as f: decomp = pickle.load(f) \n",
    "    BES_decomp = pd.read_hdf(fname+\".hdf\")\n",
    "    BES_std    = pd.read_hdf(fname+\"_std\"+\".hdf\")\n",
    "    n_components = decomp.components_.shape[0] \n",
    "    \n",
    "    \n",
    "(BES_decomp, comp_labels, comp_dict) = display_components(n_components, decomp,\n",
    "                                                          BES_std.columns, BES_decomp, manifest, \n",
    "                                                          save_folder = subdir,  \n",
    "                                                          show_first_x_comps= 5, show_histogram = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import cluster\n",
    "X = X_r\n",
    "\n",
    "two_means = cluster.MiniBatchKMeans(n_clusters=7)\n",
    "two_means.fit(X)\n",
    "\n",
    "labels = two_means.labels_\n",
    "cluster_centers = two_means.cluster_centers_\n",
    "\n",
    "labels_unique = np.unique(labels)\n",
    "n_clusters_ = len(labels_unique)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_axis = 0\n",
    "y_axis = 1\n",
    "\n",
    "xlim = np.floor( BES_decomp.loc[ :, x_axis ].min() ), np.ceil( BES_decomp.loc[ :, x_axis ].max() )\n",
    "ylim = np.floor( BES_decomp.loc[ :, y_axis ].min() ), np.ceil( BES_decomp.loc[ :, y_axis ].max() )\n",
    "\n",
    "fig = plt.figure( figsize=(16,10) )\n",
    "plt.clf()\n",
    "\n",
    "colors = cycle('bgrcmykbgrcmykbgrcmykbgrcmyk')\n",
    "for k, col in zip(range(n_clusters_), colors):\n",
    "    my_members = labels == k\n",
    "    cluster_center = cluster_centers[k]\n",
    "    plt.plot(X[my_members, x_axis], X[my_members, y_axis], col + '.', alpha=.5)\n",
    "    plt.plot(cluster_center[x_axis], cluster_center[y_axis], 'o', markerfacecolor=col,\n",
    "             markeredgecolor='k', markersize=14)\n",
    "    \n",
    "plt.xlim(xlim)\n",
    "plt.ylim(ylim)\n",
    "    \n",
    "plt.xlabel( comp_labels[x_axis] )\n",
    "plt.ylabel( comp_labels[y_axis] ) \n",
    "plt.title('Estimated number of clusters: %d' % n_clusters_);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_axis = 0\n",
    "y_axis = 2\n",
    "\n",
    "\n",
    "fig = plt.figure( figsize=(16,10) )\n",
    "plt.clf()\n",
    "\n",
    "colors = cycle('bgrcmykbgrcmykbgrcmykbgrcmyk')\n",
    "for k, col in zip(range(n_clusters_), colors):\n",
    "    my_members = labels == k\n",
    "    cluster_center = cluster_centers[k]\n",
    "    plt.plot(X[my_members, x_axis], X[my_members, y_axis], col + '.', alpha=.5)\n",
    "    plt.plot(cluster_center[x_axis], cluster_center[y_axis], 'o', markerfacecolor=col,\n",
    "             markeredgecolor='k', markersize=14)\n",
    "    \n",
    "plt.xlim(xlim)\n",
    "plt.ylim(ylim)\n",
    "    \n",
    "plt.xlabel( comp_labels[x_axis] )\n",
    "plt.ylabel( comp_labels[y_axis] ) \n",
    "plt.title('Estimated number of clusters: %d' % n_clusters_);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import numpy as np\n",
    "from sklearn.cluster import MeanShift, estimate_bandwidth\n",
    "# from sklearn.datasets.samples_generator import make_blobs\n",
    "\n",
    "X = X_r\n",
    "\n",
    "# The following bandwidth can be automatically detected using\n",
    "bandwidth = estimate_bandwidth(X, quantile=0.5, n_samples=1000)\n",
    "\n",
    "ms = MeanShift(bandwidth=bandwidth, bin_seeding=True)\n",
    "# ms = MeanShift()\n",
    "ms.fit(X)\n",
    "labels = ms.labels_\n",
    "cluster_centers = ms.cluster_centers_\n",
    "\n",
    "labels_unique = np.unique(labels)\n",
    "n_clusters_ = len(labels_unique)\n",
    "\n",
    "print(\"number of estimated clusters : %d\" % n_clusters_)\n",
    "\n",
    "\n",
    "fig = plt.figure( figsize=(16,10) )\n",
    "plt.clf()\n",
    "\n",
    "colors = cycle('bgrcmykbgrcmykbgrcmykbgrcmyk')\n",
    "for k, col in zip(range(n_clusters_), colors):\n",
    "    my_members = labels == k\n",
    "    cluster_center = cluster_centers[k]\n",
    "    plt.plot(X[my_members, x_axis], X[my_members, y_axis], col + '.')\n",
    "    plt.plot(cluster_center[x_axis], cluster_center[y_axis], 'o', markerfacecolor=col,\n",
    "             markeredgecolor='k', markersize=14)\n",
    "    \n",
    "plt.xlim(xlim)\n",
    "plt.ylim(ylim)\n",
    "    \n",
    "plt.xlabel( comp_labels[x_axis] )\n",
    "plt.ylabel( comp_labels[y_axis] ) \n",
    "plt.title('Estimated number of clusters: %d' % n_clusters_)\n",
    "# plt.show()\n",
    "\n",
    "fig.savefig(output_subfolder + decomp_method+ os.sep +\"Cluster_estimation.png\",bbox_inches='tight')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:BES_analysis]",
   "language": "python",
   "name": "conda-env-BES_analysis-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
