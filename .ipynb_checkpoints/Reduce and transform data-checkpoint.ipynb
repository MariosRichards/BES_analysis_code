{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Drop variables that don't seem to be contributing much of have problems\n",
    "# (Wave 9 variables, high (anti-/)correlation variables, mostly not filled out, very low variance, too many categories)\n",
    "# Transform variables (making some more easily readable\n",
    "# and dummying non-ordinal categorical variables to create lots of new variables)\n",
    "# rechecking if those new variables suffer from excessive levels of correlation\n",
    "\n",
    "# Output: \n",
    "# BES_reduced - imputed dataset post-transformation\n",
    "# BES_reduced_with_na - non-imputed dataset post-transformation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import gc\n",
    "import re\n",
    "\n",
    "from itertools import cycle\n",
    "from IPython.display import display\n",
    "import pickle, os\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "encoding = \"ISO-8859-1\"\n",
    "\n",
    "import Jupyter_module_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# you should clone this git to this subdirectory (in some directory - I call it BES_analysis - doesn't matter though)\n",
    "\n",
    "if os.getcwd().split(os.sep)[-1] != 'BES_analysis_code':\n",
    "    raise Exception(\"Stop! You're in the wrong directory - should be in 'BES_analysis_code'\")\n",
    "\n",
    "BES_code_folder   = \"../BES_analysis_code/\" # we should be here!\n",
    "BES_small_data_files = BES_code_folder + \"small data files\" + os.sep\n",
    "if not os.path.exists( BES_small_data_files ):\n",
    "    os.makedirs( BES_small_data_files )\n",
    "\n",
    "# we should create these if they don't already exist\n",
    "BES_data_folder   = \"../BES_analysis_data/\"\n",
    "if not os.path.exists( BES_data_folder ):\n",
    "    os.makedirs( BES_data_folder )\n",
    "\n",
    "BES_output_folder = \"../BES_analysis_output/\"\n",
    "if not os.path.exists( BES_output_folder ):\n",
    "    os.makedirs( BES_output_folder )\n",
    "    \n",
    "BES_file_manifest = pd.read_csv( BES_small_data_files + \"BES_file_manifest.csv\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BES_Panel (31328, 651)\n",
      "BES_numeric (31328, 620)\n",
      "var_type (651, 12)\n",
      "Wall time: 12 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "dataset_name = \"W4_only\"\n",
    "\n",
    "manifest = BES_file_manifest[ BES_file_manifest[\"Name\"] == dataset_name ]\n",
    "\n",
    "data_subfolder = BES_data_folder + dataset_name + os.sep\n",
    "\n",
    "filename = manifest[\"Stata_Filename\"].values[0]\n",
    "\n",
    "BES_Panel = pd.read_stata( data_subfolder + filename)\n",
    "print(\"BES_Panel\", BES_Panel.shape )\n",
    "\n",
    "\n",
    "BES_numeric = pd.read_hdf( data_subfolder + \"BESnumeric.hdf\", \"BESnumeric\" )\n",
    "print(\"BES_numeric\",  BES_numeric.shape )\n",
    "\n",
    "var_type    = pd.read_hdf( data_subfolder + \"var_type.hdf\", \"var_type\" )\n",
    "print(\"var_type\",  var_type.shape )\n",
    "\n",
    "fname = data_subfolder + \"cat_dictionary.pkl\"\n",
    "with open(fname, \"rb\") as f:\n",
    "    cat_dictionary = pickle.load( f )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BES_imputed (31328, 620)\n",
      "Wall time: 1.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# imputation_method = \"mice\" # \"mice\", \"softimpute\", \"median\", ???\n",
    "# imputed_file_name = \"BES\" + imputation_method + wave\n",
    "\n",
    "# imputation_method = \"median\" # \"mice\", \"softimpute\", \"median\", ???\n",
    "# imputed_file_name = \"BES\" + imputation_method + wave\n",
    "\n",
    "\n",
    "# BES_imputed = pd.read_hdf( BES_data_folder + imputed_file_name  + \".hdf\", imputed_file_name )\n",
    "BES_imputed = BES_numeric.fillna(BES_numeric.median())\n",
    "print(\"BES_imputed\",  BES_imputed.shape )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# England_inds = BES_Panel[BES_Panel[\"country\"]==\"England\"].index\n",
    "# print ( \"respondents not from England: \", len(BES_Panel) - len(England_inds) )\n",
    "# Remain_inds  = BES_Panel[ BES_Panel[\"euRefVote\"].cat.codes==0 ].index\n",
    "# Leave_inds   = BES_Panel[ BES_Panel[\"euRefVote\"].cat.codes==1 ].index\n",
    "# Notvoters_inds  = BES_Panel[ BES_Panel[\"euRefVote\"].cat.codes==2 ].index\n",
    "# Dontknow_inds   = BES_Panel[ BES_Panel[\"euRefVote\"].cat.codes==3 ].index\n",
    "# print ( \"respondents not Remain/Leave: \", len(BES_Panel) - len(Remain_inds) - len(Leave_inds) )\n",
    "# Leave_Remain_inds = pd.Index( np.concatenate( (Leave_inds, Remain_inds), axis=0 ) )\n",
    "# EngRemLea_inds = pd.Index( np.intersect1d( England_inds, Leave_Remain_inds ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# W9_vars = pd.Index([x for x in BES_numeric.columns if \"W9\" in x])\n",
    "# print ( \"W9 columns: \", W9_vars )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "W9_vars = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "many_cat_drop_list ['miilabel', 'profile_lea', 'profile_oslaua']\n"
     ]
    }
   ],
   "source": [
    "opts_per_cat = BES_numeric.apply(lambda x: len(pd.unique(x.dropna()))).sort_values()\n",
    "many_cat_drop_list = opts_per_cat[opts_per_cat>30].index\n",
    "# added to make sure we're only dropping *categorical* variables with lots of categories!\n",
    "many_cat_drop_list = [x for x in var_type.loc[many_cat_drop_list].index\n",
    "                      if var_type.loc[many_cat_drop_list].loc[x][\"type\"] in [3,7]]\n",
    "print(\"many_cat_drop_list\", many_cat_drop_list )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "# counts = BES_numerics_only[BES_num_and_cat[\"wave9\"]==1 ].count()\n",
    "# counts_by_individuals = BES_numerics_only[BES_num_and_cat[\"wave9\"]==1 ].count(axis=1) # lets see\n",
    "# f, axarr = plt.subplots( 2 ,figsize=(5, 5) )\n",
    "# axarr[0].hist(counts,bins=50)\n",
    "# axarr[1].hist(counts_by_individuals,bins=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# drop people who are coverage outliers\n",
    "# say top and bottom 5%\n",
    "# BES_red = BES_numerics_only.loc[BES_reduced[\"wave9\"]==1 ].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# thresh = .00\n",
    "# BES_red = BES_numeric.copy()\n",
    "# BES_red[\"coverage\"] = BES_numeric.count(axis=1)/BES_numeric.shape[1]\n",
    "\n",
    "# outlier_people = BES_red[( BES_red[\"coverage\"]>BES_red[\"coverage\"].quantile(1-thresh) ) | \n",
    "#                          ( BES_red[\"coverage\"]<BES_red[\"coverage\"].quantile(  thresh) ) ].index\n",
    "# inlier_people  = BES_red[( BES_red[\"coverage\"]<BES_red[\"coverage\"].quantile(1-thresh) ) & \n",
    "#                          ( BES_red[\"coverage\"]>BES_red[\"coverage\"].quantile(  thresh) ) ].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# columns histogrammed by %non-missing values\n",
    "# rows histogrammed by %non-missing values\n",
    "\n",
    "counts = BES_numeric.count()/BES_numeric.shape[0]\n",
    "counts_by_individuals = BES_numeric.count(axis=1)/BES_numeric.shape[1] # lets see\n",
    "f, axarr = plt.subplots( 2 ,figsize=(5, 5) )\n",
    "ax = axarr[0].hist(counts,bins=50)\n",
    "ax = axarr[1].hist(counts_by_individuals,bins=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# In theory, this is picked on the basis of looking at the above two diagrams (really, the top one!)\n",
    "# thresh = 0.33\n",
    "missing_value_thresh = 0.33\n",
    "\n",
    "mostly_not_filled_out = counts[counts<= (counts.max()*missing_value_thresh)].index\n",
    "mostly_filled_out     = counts[counts>  (counts.max()*missing_value_thresh)].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# missing_value_thresh = 0.8\n",
    "\n",
    "# mostly_not_filled_out = counts[counts<= (counts.max()*missing_value_thresh)].index\n",
    "# mostly_filled_out     = counts[counts>  (counts.max()*missing_value_thresh)].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(BES_numeric[mostly_filled_out].shape)\n",
    "missing_fraction = BES_numeric[mostly_filled_out].isnull().sum().sum() / \\\n",
    "    (BES_numeric[mostly_filled_out].shape[0]*BES_numeric[mostly_filled_out].shape[1])\n",
    "print(missing_fraction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "## 5 minutes!\n",
    "## 8h 41min 19s on full W10 panel!\n",
    "\n",
    "data = BES_numeric\n",
    "corr_mat = data.corr()\n",
    "# create lower triangle - diag mask\n",
    "col_vars = len( corr_mat )\n",
    "df = pd.DataFrame( np.arange(col_vars*col_vars).reshape(col_vars,col_vars) )\n",
    "mask = np.ones(df.shape,dtype='bool')\n",
    "mask[np.triu_indices(len(df))] = False\n",
    "corr_mat.mask(~mask).stack().hist(bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build a data frame of pairs of highly corr/anti-corr variables\n",
    "# score on correlation, %non-missing values for each\n",
    "# Building the high_corr_list is still **manual** (not much reason to automate)\n",
    "\n",
    "\n",
    "corr_df = pd.DataFrame(columns = [\"var1\",\"var2\",\"corr\",\"var1%\",\"var2%\",\"min var%\"])\n",
    "\n",
    "id_num = 0\n",
    "\n",
    "# min_filled_out = 0.02\n",
    "min_filled_out = 0.02\n",
    "\n",
    "thresh = .05\n",
    "x = ( ( corr_mat>(1-thresh) ) &mask ).values.nonzero()\n",
    "\n",
    "for i in range( 0,len(x[0]) ):\n",
    "    a = corr_mat.columns[ x[0][i] ]\n",
    "    b = corr_mat.columns[ x[1][i] ]\n",
    "    c = data[a].notnull().mean()\n",
    "    d = data[b].notnull().mean()\n",
    "    mincd = min(c,d)\n",
    "    if mincd > min_filled_out and (a not in W9_vars) and (b not in W9_vars):\n",
    "        corr_df.loc[id_num] = [ a,b,corr_mat[a][b], c, d, mincd ]\n",
    "        id_num = id_num + 1\n",
    "\n",
    "\n",
    "x = ( ( corr_mat<(thresh-1) ) &mask ).values.nonzero()\n",
    "\n",
    "for i in range( 0,len(x[0]) ):\n",
    "    a = corr_mat.columns[ x[0][i] ]\n",
    "    b = corr_mat.columns[ x[1][i] ]\n",
    "    c = data[a].notnull().mean()\n",
    "    d = data[b].notnull().mean()\n",
    "    mincd = min(c,d)\n",
    "    if (mincd > min_filled_out) and (a not in W9_vars) and (b not in W9_vars):\n",
    "        corr_df.loc[id_num] = [ a,b,corr_mat[a][b], c, d, mincd ]\n",
    "        id_num = id_num + 1\n",
    "\n",
    "corr_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# corr_df.to_hdf(BES_data_folder+\"corr_dfW10Panel.hdf\",\"corr_dfW10Panel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# corr_df = pd.read_hdf(BES_data_folder+\"corr_dfW10Panel.hdf\",\"corr_dfW10Panel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_df.sort_values(by=\"min var%\",ascending=False).head(70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_df[corr_df[\"min var%\"]>missing_value_thresh]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.linalg.matrix_rank(BES_imputed.values),BES_imputed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "high_corr_drop_list = []\n",
    "\n",
    "for ind in corr_df.index:\n",
    "    if corr_df.loc[ind][\"var1%\"] == corr_df.loc[ind][\"var2%\"]: # prefer age over ageGroup\n",
    "        options = set(corr_df.loc[ind][[\"var1\" , \"var2\"]].values)\n",
    "        if 'age' in options:\n",
    "#             print(ind, list(options.difference(['age']))[0] )\n",
    "            high_corr_drop_list.append( list(options.difference(['age']))[0] )\n",
    "    elif corr_df.loc[ind][\"var1%\"] < corr_df.loc[ind][\"var2%\"]:\n",
    "#         print(ind, corr_df.loc[ind][\"var1\"])\n",
    "        high_corr_drop_list.append( corr_df.loc[ind][\"var1\"] )\n",
    "    else:\n",
    "#         print(ind, corr_df.loc[ind][\"var2\"])\n",
    "        high_corr_drop_list.append( corr_df.loc[ind][\"var2\"] )\n",
    "    \n",
    "high_corr_drop_list = list( set(high_corr_drop_list) ) # make unique\n",
    "high_corr_drop_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# pref_list ['age','euRefVote',]\n",
    "\n",
    "\n",
    "# high_corr_drop_list = []\n",
    "\n",
    "# if ('age' in BES_numeric.columns) and ('Age' in BES_numeric.columns):\n",
    "#     high_corr_drop_list.extend('Age')\n",
    "    \n",
    "# if ('ageGroup' in BES_numeric.columns) and ('age' in BES_numeric.columns):\n",
    "#     high_corr_drop_list.extend('ageGroup')\n",
    "    \n",
    "# if ('euRefVote' in BES_numeric.columns) and ('age' in BES_numeric.columns):\n",
    "#     high_corr_drop_list.extend('ageGroup')    \n",
    "\n",
    "# high_corr_drop_list.extend(['euRefVoteUnsqueeze', 'euRefVote2', 'euRefVotePost',\n",
    "#                        'ageGroup', 'voted2015', \"ns_sec\" , 'recallVote15'])\n",
    "# # # \n",
    "# # high_corr_drop_list = ['euRefVoteUnsqueeze', 'euRefVote2', 'euRefVotePost',\n",
    "# #                        'ageGroup', 'Age', 'profile_turnout_2015', \"ns_sec\" , 'profile_past_vote_2015',\"euIDW9\"]\n",
    "\n",
    "# # high_corr_drop_list = [\"euID\",\"ageGroup\",\"euRefpastVote\",\"voted2015\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "## TEST VARIANCES\n",
    "a = BES_imputed.var()\n",
    "b = a<a.quantile(.05)\n",
    "a[b].hist(bins=100)\n",
    "very_low_var = BES_imputed.columns[b]\n",
    "# a.quantile(.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# drop columns below certain % filled out (2%)\n",
    "# drop columns which are identical (drop least filled in) or directly derived\n",
    "# make a list of derived columns??? -> scales\n",
    "\n",
    "# WHAT TO REMOVE?\n",
    "# W9_vars\n",
    "# two_opt_cats\n",
    "# many_cat_drop_list\n",
    "# high_corr_drop_list\n",
    "# mostly_not_filled_out\n",
    "# mostly_filled_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "very_low_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time to dump some variables!\n",
    "# (The results of this may have been exported back to the Imputation script\n",
    "# if it looks like nothing is happening, that's why!)\n",
    "\n",
    "\n",
    "BES_reduced = BES_imputed.copy()\n",
    "cols = BES_reduced.columns\n",
    "print(BES_reduced.shape)\n",
    "\n",
    "# # Wave 9 stuff\n",
    "# BES_reduced.drop(W9_vars,axis=1,errors='ignore',inplace=True)\n",
    "# print(\"W9_vars\", BES_reduced.shape)\n",
    "# drop_cols = cols.difference(BES_reduced.columns)\n",
    "# if ( (len(drop_cols) >0) and (len(drop_cols) <= 10) ): print(drop_cols) \n",
    "# cols = BES_reduced.columns\n",
    "\n",
    "# Categorical variables with a huge number of categories\n",
    "BES_reduced.drop(many_cat_drop_list,axis=1,errors='ignore',inplace=True)\n",
    "print(\"many_cat_drop_list\", BES_reduced.shape)\n",
    "drop_cols = cols.difference(BES_reduced.columns)\n",
    "if ( (len(drop_cols) >0) and (len(drop_cols) <= 10) ): print(drop_cols) \n",
    "cols = BES_reduced.columns\n",
    "\n",
    "# Variables which are redundant through very high correlation with other variables\n",
    "BES_reduced.drop(high_corr_drop_list,axis=1,errors='ignore',inplace=True)\n",
    "print(\"high_corr_drop_list\", BES_reduced.shape)\n",
    "drop_cols = cols.difference(BES_reduced.columns)\n",
    "if ( (len(drop_cols) >0) and (len(drop_cols) <= 10) ): print(drop_cols) \n",
    "cols = BES_reduced.columns\n",
    "\n",
    "# Variables with 'too many' missing values\n",
    "BES_reduced.drop(mostly_not_filled_out,axis=1,errors='ignore',inplace=True)\n",
    "print(\"mostly_not_filled_out\", BES_reduced.shape)\n",
    "drop_cols = cols.difference(BES_reduced.columns)\n",
    "if ( (len(drop_cols) >0) and (len(drop_cols) <= 10) ): print(drop_cols) \n",
    "cols = BES_reduced.columns\n",
    "\n",
    "# Variables with 'too low' variance\n",
    "BES_reduced.drop(very_low_var,axis=1,errors='ignore',inplace=True) \n",
    "print(\"very_low_var\", BES_reduced.shape)\n",
    "drop_cols = cols.difference(BES_reduced.columns)\n",
    "if ( (len(drop_cols) >0) and (len(drop_cols) <= 10) ): print(drop_cols) \n",
    "cols = BES_reduced.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# necessary because some imputation types impute values outside of range of available values\n",
    "# kind of suggests ... maybe not such good imputation methods??\n",
    "def float_2_categorical(series,cat_dict):\n",
    "    ser = series.astype('int')\n",
    "    ser = ser.apply( lambda x : min( x , len( cat_dict ) - 1) )\n",
    "    ser = ser.apply( lambda x : max( x , 0 ) )\n",
    "    ser = ser.apply( lambda x: cat_dict[x] )  \n",
    "    return ser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# problem\n",
    "#  'xprofile_house_tenure': 'Own â\\x80\\x93 outright',"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# reorder categories in a categorical variable so that the most numerous subcategory goes first\n",
    "# so it can be dropped automatically when dummying\n",
    "# also, return name of reference subcategory\n",
    "def select_reference_subcategory(non_ordinal):\n",
    "    order = BES_numeric[non_ordinal]\\\n",
    "        .dropna().astype('int').apply( lambda x: cat_dictionary[non_ordinal][x] ).value_counts().index\n",
    "    return BES_reduced[non_ordinal].astype('category').cat.reorder_categories( order ), order[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Recreate and rename categorical non_ordinal and ordinal variables:\n",
    "# ordinal two categories -> one single dummy binary variable labelled with positive category name (e.g gender_female)\n",
    "# ordinal 3+ category variables just renamed to append \"__top_value\" to variable name\n",
    "# non-ordinal 3+ categories -> split to individual dummy binary variables \"category_subcateory\"\n",
    "\n",
    "# need to create a dict of new_variable_names with old col names\n",
    "# to allow reinsertion of missing values, say\n",
    "new_old_col_names = dict()\n",
    "# we need to drop one sub-category for each categorical variable we dummy\n",
    "# advice supports obvious guess - always choose most numerous subcategory to the reference subcategory\n",
    "non_ordinal_base_subcat = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# NON-ORDINAL CATEGORICAL VARIABLES\n",
    "# get split into one variable for each category\n",
    "# NOTE: Dummy variable/multicollinearity trap\n",
    "# we need to choose a base variable for each category and drop it!\n",
    "\n",
    "non_ordinal_list = var_type[ var_type[\"type\"]==3 ].index.intersection(BES_reduced.columns)\n",
    "for non_ordinal in non_ordinal_list:\n",
    "    # return to text categories!\n",
    "    BES_reduced[non_ordinal] = float_2_categorical(BES_reduced[non_ordinal],\n",
    "                                                   cat_dictionary[non_ordinal])\n",
    "    # reorder to place most numerous subcategory first\n",
    "    BES_reduced[non_ordinal], non_ordinal_base_subcat[non_ordinal] = \\\n",
    "        select_reference_subcategory( non_ordinal  )\n",
    "    \n",
    "    # create dictionary so we have a record of which variable all newly generated ones come from!\n",
    "    for subcat in cat_dictionary[non_ordinal]:\n",
    "        new_col_name = non_ordinal + \"_\" + subcat\n",
    "        new_old_col_names[new_col_name] = non_ordinal\n",
    "        \n",
    "BES_reduced = pd.get_dummies( BES_reduced, columns=non_ordinal_list, drop_first=True )    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# BINARY CATEGORICAL VARIABLES\n",
    "# get renamed with 'top' value separated by single underscore\n",
    "# (this is the same as with non-ordinal variables - maybe use a different convention - like ___)\n",
    "# i.e. haveDependents -> haveDependents_No\n",
    "\n",
    "two_opt_cats = opts_per_cat[opts_per_cat==2].index.intersection(BES_reduced.columns)\n",
    "for col in two_opt_cats:\n",
    "    BES_reduced[col] = BES_reduced[col]\\\n",
    "    .astype('int').apply( lambda x: cat_dictionary[col][x] )\\\n",
    "    .astype('category').cat.reorder_categories(cat_dictionary[col])\n",
    "    \n",
    "    new_col_name = col + \"_\" + cat_dictionary[col][1]\n",
    "    new_old_col_names[new_col_name] = col\n",
    "    \n",
    "BES_reduced = pd.get_dummies( BES_reduced, columns=two_opt_cats, drop_first=True )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ORDINAL CATEGORICAL VARIABLES\n",
    "# get renamed with 'top' value separated by double underscore\n",
    "# i.e. likeBoris -> likeBoris__Strongly Agree\n",
    "\n",
    "ordinal_list = var_type[ var_type[\"type\"].apply(lambda x: x in [1,2,5]) ].index.intersection(BES_reduced.columns)\n",
    "for ordinal in ordinal_list:\n",
    "    new_col_name = ordinal + \"__\" + (cat_dictionary[ordinal])[-1:][0]\n",
    "    BES_reduced.rename(columns={ordinal:new_col_name}, inplace=True)\n",
    "    new_old_col_names[new_col_name] = ordinal\n",
    "    \n",
    "print(\"BES_reduced\",BES_reduced.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## We should do correlation testing on a version with missing values\n",
    "## and may also wish an output with missing values (e.g. to run weightedPCA on)\n",
    "## Which means reinserting missing values\n",
    "\n",
    "BES_reduced_with_na = BES_reduced.copy()\n",
    "\n",
    "for col in BES_reduced_with_na:\n",
    "    if col in new_old_col_names.keys():\n",
    "        old_col = new_old_col_names[col]\n",
    "        null_series = BES_numeric[old_col].isnull()\n",
    "    else:\n",
    "        null_series = BES_numeric[col].isnull()\n",
    "        \n",
    "    BES_reduced_with_na[col][null_series] = np.nan\n",
    "    \n",
    "    # 2 option ordinal (name change)\n",
    "    # 3+ option ordinal (name change)\n",
    "    # categorical (split into lots of variables)\n",
    "    # other (no name change)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# BES_reduced_with_na[col][~null_series]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## TEST CORRELATIONS\n",
    "# we want to avoid having columns which are basically the same data\n",
    "# through very high correlation/anti-correlation\n",
    "# and when we find them, we want to keep the column with less missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = BES_reduced_with_na\n",
    "corr_mat = data.corr()\n",
    "# create lower triangle - diag mask\n",
    "col_vars = len( corr_mat )\n",
    "df = pd.DataFrame( np.arange(col_vars*col_vars).reshape(col_vars,col_vars) )\n",
    "mask = np.ones(df.shape,dtype='bool')\n",
    "mask[np.triu_indices(len(df))] = False\n",
    "corr_mat.mask(~mask).stack().hist(bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresh = .05\n",
    "x = ( ( corr_mat>(1-thresh) ) & mask ).values.nonzero()\n",
    "\n",
    "for i in range( 0,len(x[0]) ):\n",
    "    a = corr_mat.columns[ x[0][i] ]\n",
    "    b = corr_mat.columns[ x[1][i] ]\n",
    "    print( ( a,b,corr_mat[a][b], data[a].notnull().mean(), data[b].notnull().mean() ) )\n",
    "\n",
    "x = ( ( corr_mat<(thresh-1) ) & mask ).values.nonzero()\n",
    "\n",
    "for i in range( 0,len(x[0]) ):\n",
    "    a = corr_mat.columns[ x[0][i] ]\n",
    "    b = corr_mat.columns[ x[1][i] ]\n",
    "    print( ( a,b,corr_mat[a][b], data[a].notnull().mean(), data[b].notnull().mean() ) )    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# high_corr_drop_list = ['country_Scotland', 'country_Wales', 'polKnowMay_Leader of the Labour Party',\n",
    "                      # 'polKnowBercow_Home secretary', 'polKnowMiliband_Deputy Prime Minister']\n",
    "\n",
    "dummy_cat_high_corr_drop_list = ['country_Scotland', 'country_Wales','BESwave10_Samplesurvey_taken a BES wave',\n",
    "                                'profile_ethnicity_Chinese','profile_ethnicity_Indian','profile_ethnicity_Pakistani',\n",
    "                                'profile_religion_Yes - Islam','housing_Own outright']\n",
    "\n",
    "# dummy_cat_high_corr_drop_list = ['country_Wales', 'profile_ethnicity_Chinese','profile_ethnicity_Indian','profile_ethnicity_Pakistani',\n",
    "#                                 'profile_religion_Yes - Islam']\n",
    "\n",
    "BES_reduced.drop( dummy_cat_high_corr_drop_list,axis=1, inplace=True, errors='ignore')\n",
    "BES_reduced_with_na.drop( dummy_cat_high_corr_drop_list,axis=1, inplace=True, errors='ignore')\n",
    "print(\"dummy_cat_high_corr_drop_list\", BES_reduced.shape)\n",
    "print(dummy_cat_high_corr_drop_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# very_low_var\n",
    "# NoiseVariance.loc[very_low_var].sort_values(by='noise_variance_')fc\n",
    "\n",
    "# upshot - get rid of "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# del BES_imputed, counts_by_individuals, corr_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import sys\n",
    "# for var, obj in locals().items():\n",
    "#     if ( sys.getsizeof(obj)> 1000000):\n",
    "#         print(var, sys.getsizeof(obj))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# wave = \"W10_only\"\n",
    "\n",
    "# BES_numeric = pd.read_hdf( BES_data_folder+\"BESnumeric\"+wave+\".hdf\", \"BESnumeric\"+wave )\n",
    "\n",
    "BES_reduced.to_hdf( data_subfolder + \"BES_reduced.hdf\", \"BES_reduced\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# (BES_data_folder+\"BES_reduced_with_na\"+wave+\".hdf\",\"BES_reduced_with_na\"+wave)\n",
    "BES_reduced_with_na.to_hdf( data_subfolder + \"BES_reduced_with_na.hdf\",\"BES_reduced_with_na\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save decomp object    \n",
    "fname = data_subfolder + \"new_old_col_names.pkl\"\n",
    "\n",
    "with open(fname, \"wb\") as f:\n",
    "    pickle.dump( new_old_col_names, f )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BES_reduced.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.linalg.matrix_rank( BES_reduced.drop(dummy_cat_high_corr_drop_list, axis=1, errors=\"ignore\").values )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BES_reduced.drop(dummy_cat_high_corr_drop_list,axis=1,errors=\"ignore\").shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "qr = np.linalg.qr(BES_reduced.drop(dummy_cat_high_corr_drop_list,axis=1,errors=\"ignore\").values)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BES_reduced.drop(dummy_cat_high_corr_drop_list,axis=1,errors=\"ignore\").columns[np.abs(np.sum(qr,axis=1))<.3 ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# dummy_cat_high_corr_drop_list.append('xprofile_ethnicity_Pakistani')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# dummy_cat_high_corr_drop_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# dummy_cat_high_corr_drop_list = ['country_Scotland', 'country_Wales','BESwave10_Samplesurvey_taken a BES wave',\n",
    "#                                 'profile_ethnicity_Chinese','profile_ethnicity_Indian','profile_ethnicity_Pakistani',\n",
    "#                                 'profile_religion_Yes - Islam','housing_Own outright']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# BES_reduced[\"age\"].hist(bins=85)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# a list of dropped variables!\n",
    "l = [many_cat_drop_list,\n",
    "high_corr_drop_list,\n",
    "mostly_not_filled_out,\n",
    "very_low_var]\n",
    "\n",
    "flat_list = [item for sublist in l for item in sublist]\n",
    "\n",
    "fname = data_subfolder + \"red_dropped_var\"\n",
    "with open(fname+\".pkl\", \"wb\") as f: pickle.dump( flat_list, f )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:BES_analysis]",
   "language": "python",
   "name": "conda-env-BES_analysis-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
