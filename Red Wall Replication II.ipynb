{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ec04d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Red Wall Replication II"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "524a99ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "588d7310",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"W14_comb\"\n",
    "df_list = 'BES_Panel'\n",
    "#            'BES_reduced_with_na'\n",
    "#           ]#,\"BESnumeric\"]\n",
    "\n",
    "%matplotlib inline \n",
    "%run BES_header.py {dataset_name} {df_list}\n",
    "\n",
    "del BES_Panel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0ad430c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randint\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfb09cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_identity(axes, *line_args, **line_kwargs):\n",
    "    identity, = axes.plot([], [], *line_args, **line_kwargs)\n",
    "    def callback(axes):\n",
    "        low_x, high_x = axes.get_xlim()\n",
    "        low_y, high_y = axes.get_ylim()\n",
    "        low = max(low_x, low_y)\n",
    "        high = min(high_x, high_y)\n",
    "        identity.set_data([low, high], [low, high])\n",
    "    callback(axes)\n",
    "    axes.callbacks.connect('xlim_changed', callback)\n",
    "    axes.callbacks.connect('ylim_changed', callback)\n",
    "    return axes\n",
    "\n",
    "\n",
    "def sm_and_xgb_plot(var_stub,title,specific_vars,df_simp,geomask=None,window=None,dependence_plots=False):\n",
    "    from scipy import stats\n",
    "    Treatment = var_stub\n",
    "    var_list = [var_stub]\n",
    "    var_stub_list = [var_stub,]\n",
    "    df_simp[var_stub] = BES_census_data[var_stub]\n",
    "    mask = df_simp[var_stub].notnull()\n",
    "    if geomask is not None:\n",
    "        mask = mask&geomask\n",
    "    \n",
    "    min_features = 30\n",
    "    colname = var_stub\n",
    "#     dependence_plots = True\n",
    "\n",
    "    alg = get_xgboost_alg(classification_problem=False)\n",
    "    (explainer, shap_values, train_columns, train_index, alg,output_subfolder)=\\\n",
    "        xgboost_run(subdir=colname,dataset=df_simp[mask][specific_vars+[var_stub]],\n",
    "                var_list=var_list,var_stub_list=var_stub_list,\n",
    "                use_specific_weights=None,\n",
    "                min_features = min(df_simp.shape[1]-1,min_features),verbosity=0,\n",
    "                skip_bar_plot=True,dependence_plots=dependence_plots,alg=alg,eval_metric=eval_metric,                    \n",
    "                title = title)\n",
    "    xgboost_pred = pd.Series(alg.predict(df_simp[train_columns]),index=df_simp.index)\n",
    "\n",
    "    import statsmodels.api as sm\n",
    "    Y = df_simp[mask][var_stub]\n",
    "    ## default mean imputation!\n",
    "    X = df_simp[mask][specific_vars].astype('float').fillna(df_simp[mask][specific_vars].mean())\n",
    "#     X = sm.add_constant(X)\n",
    "    X[\"const\"]=1.0\n",
    "    model = sm.OLS(Y,X)\n",
    "    results = model.fit()\n",
    "    Xnew = df_simp[train_columns].astype('float')\n",
    "#     Xnew = sm.add_constant(Xnew)\n",
    "    Xnew[\"const\"]=1.0\n",
    "    statsmodels_pred = model.predict(results.params,Xnew)\n",
    "    display( results.summary() );\n",
    "    #########################################################\n",
    "\n",
    "    ons_ids = BES_census_data.loc[df_simp.index,\"ONSConstID\"]\n",
    "#     if geomask is not None:\n",
    "#         ons_ids = ons_ids[geomask]\n",
    "\n",
    "    xgb_var = title+\"_xgb\"+\"_und\"\n",
    "    sm_var = title+\"_sm\"+\"_und\"\n",
    "\n",
    "#     print(\"!!!!!!!!!!!!!!!!!!!!!!\")\n",
    "#     print(xgb_var,sm_var,title)\n",
    "    merged[title+\"_xgb\"]=np.nan\n",
    "    merged.loc[ ons_ids , title+\"_xgb\" ] = xgboost_pred.values\n",
    "    merged[xgb_var] = merged[var_stub] - merged[title+\"_xgb\"]\n",
    "\n",
    "    merged[title+\"_sm\"]=np.nan\n",
    "    merged.loc[ ons_ids , title+\"_sm\" ] = statsmodels_pred\n",
    "    merged[sm_var] = merged[var_stub] - merged[title+\"_sm\"]\n",
    "\n",
    "    merged_mask = merged.reset_index()['PCON13CD'].isin(ons_ids[mask]).values\n",
    "    \n",
    "    \n",
    "\n",
    "    fig, axes = plt.subplots(nrows=1,ncols=5,figsize=(16.5,4))\n",
    "    merged[xgb_var][merged_mask].hist(bins=50,ax=axes[0])\n",
    "#     mask = merged[xgb_var].notnull()\n",
    "    sns.regplot(merged[title+\"_xgb\"][merged_mask],merged[var_stub][merged_mask],ax=axes[1])\n",
    "    r, p, n= corr_simple_pearsonr(merged[title+\"_xgb\"],merged[var_stub], mask=merged_mask, round_places=2)\n",
    "    axes[1].text(0.05, .9, 'r={:.2f}, p={:.2g}'.format(r, round(p,4)),verticalalignment='top', transform=axes[1].transAxes);\n",
    "    add_identity(axes[1], color='r', ls='--')\n",
    "\n",
    "    merged[sm_var][merged_mask].hist(bins=50,ax=axes[2])\n",
    "#     mask = merged[sm_var].notnull()\n",
    "    sns.regplot(merged[title+\"_sm\"][merged_mask],merged[var_stub][merged_mask],ax=axes[3])\n",
    "    corr_mask = merged[title+\"_sm\"][merged_mask].notnull()&merged[var_stub][merged_mask].notnull()\n",
    "    r, p, n= corr_simple_pearsonr(merged[title+\"_sm\"],merged[var_stub], mask=merged_mask, round_places=2)\n",
    "#     r, p = stats.pearsonr(  merged[title+\"_sm\"][merged_mask][corr_mask]merged[var_stub][merged_mask][corr_mask] )\n",
    "    axes[3].text(0.05, .9, 'r={:.2f}, p={:.2g}'.format(r, round(p,4)),verticalalignment='top', transform=axes[3].transAxes);\n",
    "    add_identity(axes[3], color='r', ls='--')\n",
    "\n",
    "    axes[0].set_ylabel(\"freq\");\n",
    "    axes[0].set_xlabel(\"underperf vs model (xgb)\");\n",
    "    axes[1].set_ylabel(var_stub);\n",
    "    axes[1].set_xlabel(var_stub+\" pred (xgb)\");\n",
    "    axes[2].set_ylabel(\"freq\");\n",
    "    axes[2].set_xlabel(\"underperf vs model (sm)\");\n",
    "    axes[3].set_ylabel(var_stub);\n",
    "    axes[3].set_xlabel(var_stub+\" pred (sm)\");\n",
    "\n",
    "    \n",
    "    g = sns.regplot(x=merged[xgb_var][merged_mask], y=merged[sm_var][merged_mask], ax = axes[4])\n",
    "#     r, p = stats.pearsonr( merged[xgb_var][merged_mask] , merged[sm_var][merged_mask] )\n",
    "    r, p, n= corr_simple_pearsonr(merged[xgb_var],merged[sm_var], mask=merged_mask, round_places=2)\n",
    "    axes[4].text(0.05, .9, 'r={:.2f}, p={:.2g}'.format(r, round(p,4)),verticalalignment='top', transform=axes[4].transAxes);\n",
    "    axes[4].set_ylabel(\"underperf vs model (sm)\");\n",
    "    axes[4].set_xlabel(\"underperf vs model (xgb)\");\n",
    "    add_identity(axes[4], color='r', ls='--')\n",
    "    plt.suptitle(\"Residual histograms and regression plots for each of Xgboost/Statsmodels models\\nFollowed by regression of each models residuals\");\n",
    "    ##########################################################\n",
    "\n",
    "    top75 = False\n",
    "    add_colorbar = True\n",
    "\n",
    "    if top75 == True:\n",
    "        merged[xgb_var] = merged[xgb_var]<merged[xgb_var].quantile(.25)\n",
    "        merged[sm_var] = merged[sm_var]<merged[sm_var].quantile(.25)\n",
    "    else:\n",
    "        merged[xgb_var] = -merged[xgb_var]\n",
    "        merged[sm_var] = -merged[sm_var]\n",
    "    if window is not None:\n",
    "        merged_window = merged.reset_index()['PCON13CD'].isin(ons_ids[window]).values\n",
    "        merged.loc[~merged_window,xgb_var]=np.nan\n",
    "        merged.loc[~merged_window,sm_var]=np.nan\n",
    "    else:\n",
    "        merged.loc[~merged_mask,xgb_var]=np.nan\n",
    "        merged.loc[~merged_mask,sm_var]=np.nan\n",
    "              \n",
    "\n",
    "    suptitle = \"2017GE: Conservative Performance\\nRelative To Demographic Model\"\n",
    "    symmetrical_colour_scale = True\n",
    "    # set the range for the choropleth\n",
    "    vmin, vmax = np.floor(merged[[xgb_var,sm_var]].min().min()/10)*10, np.ceil(merged[[xgb_var,sm_var]].max().max()/10)*10\n",
    "    \n",
    "    if symmetrical_colour_scale:\n",
    "        max_abs_scale = max(abs(vmin),abs(vmax))\n",
    "        vmin = (vmin/abs(vmin))*max_abs_scale\n",
    "        vmax = (vmax/abs(vmax))*max_abs_scale\n",
    "\n",
    "    # create figure and axes for Matplotlib\n",
    "    fig, ax = plt.subplots(nrows=1,ncols=2, figsize=(16, 10))\n",
    "\n",
    "    # create map\n",
    "    merged.plot(column=xgb_var, cmap='seismic', norm=plt.Normalize(vmin=vmin, vmax=vmax), linewidth=0.1, ax=ax[0], edgecolor='0.8',aspect=1)\n",
    "    # remove the axis\n",
    "    ax[0].axis('off')\n",
    "    # add a title\n",
    "    ax[0].set_title(suptitle+\"(xgb)\", \\\n",
    "                  fontdict={'fontsize': '18',\n",
    "                            'fontweight' : '3'})\n",
    "\n",
    "    # create map\n",
    "    merged.plot(column=sm_var,  cmap='seismic', norm=plt.Normalize(vmin=vmin, vmax=vmax), linewidth=0.1, ax=ax[1], edgecolor='0.8',aspect=1)\n",
    "    # remove the axis\n",
    "    ax[1].axis('off')\n",
    "    # add a title\n",
    "    ax[1].set_title(suptitle+\"(sm)\", \\\n",
    "                  fontdict={'fontsize': '18',\n",
    "                            'fontweight' : '3'})\n",
    "\n",
    "\n",
    "\n",
    "    # create an annotation for the  data source\n",
    "    plt.annotate('Source: https://www.britishelectionstudy.com/data-objects/linked-data/',\n",
    "               xy=(0.3, .17), xycoords='figure fraction',\n",
    "               verticalalignment='bottom',\n",
    "                 #horizontalalignment='middle', \n",
    "               fontsize=10, color='#555555')\n",
    "\n",
    "    if add_colorbar:\n",
    "        # Create colorbar as a legend\n",
    "        sm = plt.cm.ScalarMappable(cmap=plt.cm.get_cmap('seismic').reversed(),#'seismic',\n",
    "                                   norm=plt.Normalize(vmin=vmin, vmax=vmax))\n",
    "#         sm._A = []\n",
    "        cbar = fig.colorbar(sm,\n",
    "#                             pad=-.5,\n",
    "#                             orientation='horizontal',\n",
    "                            pad=0,\n",
    "                            location='top',ax=ax[0:2], shrink=.2,fraction=.3)\n",
    "        # location='bottom',\n",
    "\n",
    "    Treatment = \"Chloropleth\"\n",
    "    output_subfolder = create_subdir(BES_output_folder,Treatment)\n",
    "    fname = output_subfolder +suptitle +\".png\"\n",
    "    # this will save the figure as a high-res png. you can also save as svg\n",
    "    plt.savefig( fname, bbox_inches='tight', dpi=1000 )\n",
    "    return ()\n",
    "\n",
    "def model_comparison(var_stub,title,specific_vars_list,df_simp,geomask=None,model_type='sm',window=None,dependence_plots=False):\n",
    "    from scipy import stats\n",
    "    Treatment = var_stub\n",
    "    var_list = [var_stub]\n",
    "    var_stub_list = [var_stub,]\n",
    "    df_simp[var_stub] = BES_census_data[var_stub]\n",
    "    mask = df_simp[var_stub].notnull()\n",
    "    if geomask is not None:\n",
    "        mask = mask&geomask\n",
    "    ons_ids = BES_census_data.loc[df_simp.index,\"ONSConstID\"]    \n",
    "#     print(\"MASK DEBUG\")\n",
    "#     print(mask.sum())\n",
    "    ######\n",
    "    count=1\n",
    "#     model_list = []\n",
    "    for specific_vars in specific_vars_list:\n",
    "        print(\"model\"+str(count))\n",
    "        min_features = 30\n",
    "        colname = var_stub\n",
    "#         dependence_plots = True\n",
    "        alg = get_xgboost_alg(classification_problem=False)\n",
    "        (explainer, shap_values, train_columns, train_index, alg,output_subfolder)=\\\n",
    "            xgboost_run(subdir=colname,dataset=df_simp[mask][specific_vars+[var_stub]],\n",
    "                    var_list=var_list,var_stub_list=var_stub_list,\n",
    "                    use_specific_weights=None,\n",
    "                    min_features = min(df_simp.shape[1]-1,min_features),verbosity=0,\n",
    "                    skip_bar_plot=True,dependence_plots=dependence_plots,alg=alg,eval_metric=eval_metric,                    \n",
    "                    title = title)\n",
    "        xgboost_pred = pd.Series(alg.predict(df_simp[train_columns]),index=df_simp.index)\n",
    "        ######\n",
    "        import statsmodels.api as sm\n",
    "        Y = df_simp[mask][var_stub]\n",
    "        X = df_simp[mask][specific_vars].astype('float').fillna(df_simp[mask][specific_vars].mean())\n",
    "#         X = sm.add_constant(X)\n",
    "        X[\"const\"]=1.0\n",
    "        model = sm.OLS(Y,X)\n",
    "        results = model.fit()\n",
    "        Xnew = df_simp[train_columns].astype('float')\n",
    "#         Xnew = sm.add_constant(Xnew)\n",
    "        Xnew[\"const\"]=1.0\n",
    "        statsmodels_pred = model.predict(results.params,Xnew)\n",
    "        display( results.summary() );\n",
    "        ######\n",
    "        pred_name = title+\"_model\"+str(count)\n",
    "        merged[pred_name]=np.nan\n",
    "        if model_type == \"xgb\":\n",
    "            merged.loc[ ons_ids , pred_name ] = xgboost_pred.values\n",
    "        else:\n",
    "            merged.loc[ ons_ids , pred_name ] = statsmodels_pred\n",
    "        merged[pred_name+\"_und\"] = merged[var_stub] - merged[pred_name]\n",
    "        \n",
    "        count=count+1\n",
    "\n",
    "    \n",
    "    merged_mask = merged.reset_index()['PCON13CD'].isin(ons_ids[mask]).values\n",
    "    \n",
    "\n",
    "    if len(specific_vars_list)==2:\n",
    "    \n",
    "        fig, axes = plt.subplots(nrows=1,ncols=5,figsize=(16.5,4))\n",
    "        merged[title+\"_model1\"+\"_und\"][merged_mask].hist(bins=50,ax=axes[0])\n",
    "    #     mask = merged[xgb_var].notnull()\n",
    "#         model1 = title+\"_model1\"\n",
    "#         model2 = title+\"_model2\"\n",
    "        sns.regplot(merged[title+\"_model1\"][merged_mask],merged[var_stub][merged_mask],ax=axes[1])\n",
    "#         r, p = stats.pearsonr( merged[title+\"_model1\"][merged_mask] , merged[var_stub][merged_mask] )\n",
    "        r, p, n= corr_simple_pearsonr(merged[title+\"_model1\"],merged[var_stub], mask=merged_mask, round_places=2)\n",
    "        axes[1].text(0.05, .9, 'r={:.2f}, p={:.2g}'.format(r, round(p,4)),verticalalignment='top', transform=axes[1].transAxes);\n",
    "        add_identity(axes[1], color='r', ls='--')\n",
    "        \n",
    "        merged[title+\"_model2\"+\"_und\"][merged_mask].hist(bins=50,ax=axes[2])\n",
    "    #     mask = merged[sm_var].notnull()\n",
    "        sns.regplot(merged[title+\"_model2\"][merged_mask],merged[var_stub][merged_mask],ax=axes[3])\n",
    "#         r, p = stats.pearsonr( merged[title+\"_model2\"][merged_mask] , merged[var_stub][merged_mask] )\n",
    "        r, p, n= corr_simple_pearsonr(merged[title+\"_model2\"],merged[var_stub], mask=merged_mask, round_places=2)\n",
    "        axes[3].text(0.05, .9, 'r={:.2f}, p={:.2g}'.format(r, round(p,4)),verticalalignment='top', transform=axes[3].transAxes);\n",
    "        add_identity(axes[3], color='r', ls='--')\n",
    "\n",
    "        axes[0].set_ylabel(\"freq\");\n",
    "        axes[0].set_xlabel(\"model 1 residuals\");\n",
    "        axes[1].set_ylabel(var_stub);\n",
    "        axes[1].set_xlabel(var_stub+\" model 1 pred\");\n",
    "        axes[2].set_ylabel(\"freq\");\n",
    "        axes[2].set_xlabel(\"model 2 residuals\");\n",
    "        axes[3].set_ylabel(var_stub);\n",
    "        axes[3].set_xlabel(var_stub+\" model 2 pred\");\n",
    "\n",
    "        g = sns.regplot(x=merged[title+\"_model1\"+\"_und\"][merged_mask],\n",
    "                        y=merged[title+\"_model2\"+\"_und\"][merged_mask], ax = axes[4])\n",
    "#         r, p = stats.pearsonr( merged[title+\"_model1\"+\"_und\"][merged_mask],\n",
    "#                                merged[title+\"_model2\"+\"_und\"][merged_mask] )\n",
    "        r, p, n= corr_simple_pearsonr(merged[title+\"_model1\"],merged[title+\"_model2\"+\"_und\"], mask=merged_mask, round_places=2)\n",
    "        axes[4].set_ylabel(\"model 1 residuals\");\n",
    "        axes[4].set_xlabel(\"model 2 residuals\");\n",
    "        axes[4].text(0.05, .9, 'r={:.2f}, p={:.2g}'.format(r, round(p,4)),verticalalignment='top', transform=axes[4].transAxes);\n",
    "        add_identity(axes[4], color='r', ls='--')\n",
    "        \n",
    "        fig.tight_layout()\n",
    "#         plt.suptitle(\"Residual histograms and regression plots for each of Xgboost/Statsmodels models\\nFollowed by regression of each models residuals\");\n",
    "    ##########################################################\n",
    "\n",
    "    top75 = False\n",
    "    fraction = .25\n",
    "    add_colorbar = True\n",
    "#     plt_title = \"2017GE: Conservative Performance\\nRelative To Demographic Model\"\n",
    "    symmetrical_colour_scale = True    \n",
    "    \n",
    "    var_list = [title+\"_model\"+str(x)+\"_und\" for x in range(1,len(specific_vars_list)+1)]\n",
    "    \n",
    "    for resid in var_list:\n",
    "        if top75 == True:\n",
    "            merged[resid] = merged[resid]<merged[resid].quantile(fraction)\n",
    "        else:\n",
    "            merged[resid] = -merged[resid]\n",
    "        if window is not None:\n",
    "            merged_window = merged.reset_index()['PCON13CD'].isin(ons_ids[window]).values\n",
    "            merged.loc[~merged_window,resid]=np.nan\n",
    "        else:\n",
    "            merged.loc[~merged_mask,resid]=np.nan\n",
    "\n",
    "\n",
    "\n",
    "    # set the range for the choropleth\n",
    "    vmin, vmax = np.floor(merged[var_list].min().min()/10)*10, np.ceil(merged[var_list].max().max()/10)*10\n",
    "    if symmetrical_colour_scale:\n",
    "        max_abs_scale = max(abs(vmin),abs(vmax))\n",
    "        vmin = (vmin/abs(vmin))*max_abs_scale\n",
    "        vmax = (vmax/abs(vmax))*max_abs_scale\n",
    "\n",
    "    # create figure and axes for Matplotlib\n",
    "    fig, ax = plt.subplots(nrows=1,ncols=len(specific_vars_list),\n",
    "                           figsize=(16, 10))\n",
    "    \n",
    "    for var_no in range(len(var_list)):\n",
    "        # create map\n",
    "        merged.plot(column=var_list[var_no], cmap='seismic', norm=plt.Normalize(vmin=vmin, vmax=vmax), linewidth=0.1,\n",
    "                    ax=ax[var_no], edgecolor='0.8',aspect=1)\n",
    "\n",
    "        # remove the axis\n",
    "        ax[var_no].axis('off')\n",
    "        # add a title\n",
    "        ax[var_no].set_title(\"model\"+str(var_no+1), \\\n",
    "                      fontdict={'fontsize': '18',\n",
    "                                'fontweight' : '3'})\n",
    "    \n",
    "#     # create map\n",
    "#     merged.plot(column=sm_var,  cmap='seismic', norm=plt.Normalize(vmin=vmin, vmax=vmax), linewidth=0.1, ax=ax[1], edgecolor='0.8',aspect=1)\n",
    "#     # remove the axis\n",
    "#     ax[1].axis('off')\n",
    "#     # add a title\n",
    "#     ax[1].set_title(title+\"(sm)\", \\\n",
    "#                   fontdict={'fontsize': '18',\n",
    "#                             'fontweight' : '3'})\n",
    "\n",
    "\n",
    "\n",
    "    # create an annotation for the  data source\n",
    "    plt.annotate('Source: https://www.britishelectionstudy.com/data-objects/linked-data/',\n",
    "               xy=(0.3, .17), xycoords='figure fraction',\n",
    "               verticalalignment='bottom',\n",
    "                 #horizontalalignment='middle', \n",
    "               fontsize=10, color='#555555')\n",
    "\n",
    "    if add_colorbar:\n",
    "        # Create colorbar as a legend\n",
    "        sm = plt.cm.ScalarMappable(cmap=plt.cm.get_cmap('seismic').reversed(),#'seismic',\n",
    "                                   norm=plt.Normalize(vmin=vmin, vmax=vmax))\n",
    "#         sm._A = []\n",
    "        if len(specific_vars_list)==2:\n",
    "            location = 'top'\n",
    "        else:\n",
    "            location = 'bottom'\n",
    "        cbar = fig.colorbar(sm,\n",
    "#                             pad=-.5,\n",
    "#                             orientation='horizontal',\n",
    "                            pad=0,\n",
    "                            location=location,ax=ax[0:len(specific_vars_list)], shrink=.2,fraction=.3)\n",
    "        # location='bottom',\n",
    "\n",
    "#     plt.suptitle(title)\n",
    "        \n",
    "    Treatment = \"Chloropleth\"\n",
    "    output_subfolder = create_subdir(BES_output_folder,Treatment)\n",
    "    fname = output_subfolder +title +\".png\"\n",
    "    # this will save the figure as a high-res png. you can also save as svg\n",
    "    plt.savefig( fname, bbox_inches='tight', dpi=1000 )\n",
    "    return ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b78ab3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "BES_census_data_2019 = pd.read_stata( BES_small_data_files + \"BES-2019-General-Election-results-file-v1.0.dta\" )\n",
    "print(\"BES_census_data 2019\", BES_census_data_2019.shape )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82e8da21",
   "metadata": {},
   "outputs": [],
   "source": [
    "BES_census_data_2017 = pd.read_stata( BES_small_data_files + \"BES-2017-General-Election-results-file-v1.0.dta\" )\n",
    "print(\"BES_census_data\", BES_census_data_2017.shape )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9d4cc78",
   "metadata": {},
   "outputs": [],
   "source": [
    "BES_census_data_2015 = pd.read_stata( BES_small_data_files + \"BES-2015-General-Election-results-file-v2.21.dta\" )\n",
    "print(\"BES_census_data 2015\", BES_census_data_2015.shape )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59bf0bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "BES_census_data = BES_census_data_2019.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8afc11ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### BUG! -> check if in latest release, if yes, then tell them! ###########\n",
    "BES_census_data.loc[BES_census_data[\"Country\"].isin([\"Scotland\"]),\"c11Households\"] =\\\n",
    "    BES_census_data.loc[BES_census_data[\"Country\"].isin([\"Scotland\"]),\"c11Households\"]*100\n",
    "BES_census_data[\"ConstituencyName\"] = BES_census_data[\"ConstituencyName\"].replace(\"Ynys Môn\",\"Ynys Mon\")\n",
    "BES_census_data[\"Winner15\"] = BES_census_data[\"Winner15\"].replace(\"Speaker.\",\"Speaker\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c60da951",
   "metadata": {},
   "outputs": [],
   "source": [
    "party_19_list = search(BES_census_data,\"^(Con|Lab|Brexit|LD|UKIP|SNP|PC|Green)19$\").index\n",
    "BES_census_data[[x+\"_elec\" for x in party_19_list]] = BES_census_data[party_19_list].apply(lambda x: x*BES_census_data[\"Turnout19\"]/100).fillna(0)\n",
    "BES_census_data[\"DNV19_elec\"] = 100-BES_census_data[\"Turnout19\"]\n",
    "BES_census_data[\"DNV&OTH19_elec\"] = 100-BES_census_data[\"Turnout19\"]-BES_census_data[\"Other19\"].fillna(0)\n",
    "BES_census_data[\"Auth_Right19_elec\"] = BES_census_data[\"Con19_elec\"]+BES_census_data[\"UKIP19_elec\"]+BES_census_data[\"Brexit19_elec\"]\n",
    "BES_census_data[\"Lib_Left19_elec\"] = BES_census_data[\"Lab19_elec\"]+BES_census_data[\"LD19_elec\"]+BES_census_data[\"PC19_elec\"]+BES_census_data[\"SNP19_elec\"]+BES_census_data[\"Green19_elec\"]\n",
    "\n",
    "party_17_list = search(BES_census_data,\"^(Con|Lab|Brexit|LD|UKIP|SNP|PC|Green)17$\").index\n",
    "BES_census_data[[x+\"_elec\" for x in party_17_list]] = BES_census_data[party_17_list].apply(lambda x: x*BES_census_data[\"Turnout17\"]/100).fillna(0)\n",
    "BES_census_data[\"DNV17_elec\"] = 100-BES_census_data[\"Turnout17\"]\n",
    "BES_census_data[\"DNV&OTH17_elec\"] = 100-BES_census_data[\"Turnout17\"]-BES_census_data[\"Other17\"].fillna(0)\n",
    "BES_census_data[\"Auth_Right17_elec\"] = BES_census_data[\"Con17_elec\"]+BES_census_data[\"UKIP17_elec\"]#+BES_census_data[\"Brexit17_elec\"]\n",
    "BES_census_data[\"Lib_Left17_elec\"] = BES_census_data[\"Lab17_elec\"]+BES_census_data[\"LD17_elec\"]+BES_census_data[\"PC17_elec\"]\\\n",
    "                                    +BES_census_data[\"SNP17_elec\"]+BES_census_data[\"Green17_elec\"]\n",
    "\n",
    "party_15_list = search(BES_census_data,\"^(Con|Lab|Brexit|LD|UKIP|SNP|PC|Green)15$\").index\n",
    "BES_census_data[[x+\"_elec\" for x in party_15_list]] = BES_census_data[party_15_list]\\\n",
    "                                    .apply(lambda x: x*BES_census_data[\"Turnout15\"]/100).fillna(0)\n",
    "BES_census_data[\"DNV15_elec\"] = 100-BES_census_data[\"Turnout15\"]\n",
    "BES_census_data[\"DNV&OTH15_elec\"] = 100-BES_census_data[\"Turnout15\"]-BES_census_data[\"Other15\"].fillna(0)\n",
    "BES_census_data[\"Auth_Right15_elec\"] = BES_census_data[\"Con15_elec\"]+BES_census_data[\"UKIP15_elec\"]#+BES_census_data[\"Brexit17_elec\"]\n",
    "BES_census_data[\"Lib_Left15_elec\"] = BES_census_data[\"Lab15_elec\"]+BES_census_data[\"LD15_elec\"]+BES_census_data[\"PC15_elec\"]\\\n",
    "                                    +BES_census_data[\"SNP15_elec\"]+BES_census_data[\"Green15_elec\"]\n",
    "\n",
    "party_10_list = search(BES_census_data,\"^(Con|Lab|BNP|LD|UKIP|SNP|PC|Green)10$\").index\n",
    "BES_census_data[[x+\"_elec\" for x in party_10_list]] = BES_census_data[party_10_list].apply(lambda x: x*BES_census_data[\"Turnout10\"]/100).fillna(0)\n",
    "BES_census_data[\"DNV10_elec\"] = 100-BES_census_data[\"Turnout10\"]\n",
    "BES_census_data[\"DNV&OTH10_elec\"] = 100-(BES_census_data[party_10_list].apply(lambda x: x*BES_census_data[\"Turnout10\"]/100).sum(axis=1))\n",
    "BES_census_data[\"Auth_Right10_elec\"] = BES_census_data[\"Con10_elec\"]+BES_census_data[\"UKIP10_elec\"]+BES_census_data[\"BNP10_elec\"]\n",
    "BES_census_data[\"Lib_Left10_elec\"] = BES_census_data[\"Lab10_elec\"]+BES_census_data[\"LD10_elec\"]+BES_census_data[\"PC10_elec\"]\\\n",
    "                                    +BES_census_data[\"SNP10_elec\"]+BES_census_data[\"Green10_elec\"]\n",
    "\n",
    "party_05_list = search(BES_census_data,\"^(Con|Lab|BNP|LD|UKIP|SNP|PC|Green)05$\").index\n",
    "BES_census_data[[x+\"_elec\" for x in party_05_list]] = BES_census_data[party_05_list].apply(lambda x: x*BES_census_data[\"Turnout05\"]/100).fillna(0)\n",
    "BES_census_data[\"DNV05_elec\"] = 100-BES_census_data[\"Turnout05\"]\n",
    "BES_census_data[\"DNV&OTH05_elec\"] = 100-BES_census_data[\"Turnout05\"]-BES_census_data[\"Other05\"].fillna(0)\n",
    "BES_census_data[\"Auth_Right05_elec\"] = BES_census_data[\"Con05_elec\"]+BES_census_data[\"UKIP05_elec\"]+BES_census_data[\"BNP05_elec\"]\n",
    "BES_census_data[\"Lib_Left05_elec\"] = BES_census_data[\"Lab05_elec\"]+BES_census_data[\"LD05_elec\"]+BES_census_data[\"PC05_elec\"]\\\n",
    "                                    +BES_census_data[\"SNP05_elec\"]+BES_census_data[\"Green05_elec\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e24a76be",
   "metadata": {},
   "outputs": [],
   "source": [
    "BES_census_data[search(BES_census_data_2015,\"SpendPercent\").index] = BES_census_data_2015[search(BES_census_data_2015,\"SpendPercent\").index]\n",
    "\n",
    "BES_census_data[['SNPLongSpendPercent', 'SNPShortSpendPercent',\n",
    "       'PCLongSpendPercent', 'PCShortSpendPercent']] = BES_census_data[['SNPLongSpendPercent', 'SNPShortSpendPercent',\n",
    "       'PCLongSpendPercent', 'PCShortSpendPercent']].replace(\".\",np.nan).astype('float')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3e3ac72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# search(BES_census_data_2019,\"05\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f28207c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40325bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "occ_df = pd.read_excel( open(BES_small_data_files+'Census2011-ConstituencyProfile-uk.xlsx','rb'), sheet_name='KS608D' )\n",
    "occ_df.columns = occ_df.loc[3]\n",
    "occ_df = occ_df.drop([0,1,2,3,654,])\n",
    "# occ_df\n",
    "\n",
    "var_list = [\"CON%ELEM\",\"CON%OPS\",\"CON%SAL_SERV\",\"CON%CAR_LEI_\",\"CON%SKILL\",\"CON%ADMIN_SEC\",\"CON%ASSOC_PROF_TECH\",\n",
    " \"CON%PROF\",\"CON%MAN_DIR_SEN\"]\n",
    "\n",
    "# occ_df = occ_df.set_index(\"ONSConstID\").loc[merged.index]\n",
    "# var_list = [\"CON%ELEM\",\"CON%OPS\",\"CON%SAL_SERV\",\"CON%CAR_LEI_\",\"CON%SKILL\",\"CON%ADMIN_SEC\",\"CON%ASSOC_PROF_TECH\",\n",
    "#  \"CON%PROF\",\"CON%MAN_DIR_SEN\"]\n",
    "\n",
    "BES_census_data[var_list] = occ_df.set_index(\"ONSConstID\").loc[BES_census_data[\"ONSConstID\"]][var_list].values\n",
    "\n",
    "merseyside = [\"Birkenhead\",\"Bootle\",\"Garston and Halewood\",\"Knowsley\",\n",
    "             \"Liverpool, Riverside\",\"Liverpool, Walton\",\"Liverpool, Wavertree\",\"Liverpool, West Derby\",\n",
    "             \"Sefton Central\",\"Southport\",\"St Helens North\",\"St Helens South and Whiston\",\n",
    "             \"Wallasey\",\"Wirral South\",\"Wirral West\"]\n",
    "\n",
    "BES_census_data[\"Merseyside\"] = BES_census_data[\"ConstituencyName\"].isin(merseyside).astype('float')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f43a6cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "welsh_df = pd.read_excel( open(BES_small_data_files+'welsh_constituency_data.xls','rb'), sheet_name='WelshLanguage' )\n",
    "welsh_df.columns = welsh_df.loc[4]\n",
    "welsh_df = welsh_df.drop([0,1,2,3,4,50,51,52])\n",
    "welsh_df = welsh_df.replace(\"Ynys Môn\",\"Ynys Mon\")\n",
    "welsh_df = welsh_df.set_index(\"Assembly Constituency Area\")\n",
    "BES_census_data[\"speakWelsh\"] = [welsh_df.loc[x,\"Percentage aged 3+ who can speak Welsh\"] if x in welsh_df.index else 0 for x in BES_census_data[\"ConstituencyName\"]  ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0ca86b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "BES_census_data[\"c11DeprivedMean\"] = (BES_census_data[\"c11Deprived1\"]+(BES_census_data[\"c11Deprived2\"]*2)+(BES_census_data[\"c11Deprived3\"]*3)+(BES_census_data[\"c11Deprived4\"]*4))/100\n",
    "BES_census_data[\"c11CarsMean\"] = (BES_census_data[\"c11CarsOne\"]+(BES_census_data[\"c11CarsTwo\"]*2)+(BES_census_data[\"c11CarsThree\"]*3)+(BES_census_data[\"c11CarsFour\"]*4))/100\n",
    "BES_census_data[\"c11AgeMean\"] = (\n",
    "                                 BES_census_data[\"c11Age18to19\"]*18.5+\\\n",
    "                                 BES_census_data[\"c11Age20to24\"]*22+\\\n",
    "                                 BES_census_data[\"c11Age25to29\"]*27+\\\n",
    "                                 BES_census_data[\"c11Age30to44\"]*37+\\\n",
    "                                 BES_census_data[\"c11Age45to59\"]*52+\\\n",
    "                                 BES_census_data[\"c11Age60to64\"]*62+\\\n",
    "                                 BES_census_data[\"c11Age65to74\"]*69.5+\\\n",
    "                                 BES_census_data[\"c11Age75to84\"]*79.5+\\\n",
    "                                 BES_census_data[\"c11Age85to89\"]*87+\\\n",
    "                                 BES_census_data[\"c11Age90plus\"]*95  )/100\n",
    "BES_census_data[\"c11DeprivedHigh\"] =BES_census_data[\"c11Deprived3\"]+BES_census_data[\"c11Deprived4\"]\n",
    "BES_census_data[\"c11DeprivedLow\"] =BES_census_data[\"c11Deprived2\"]+BES_census_data[\"c11Deprived1\"]+BES_census_data[\"c11DeprivedNone\"]\n",
    "## kids\n",
    "BES_census_data[\"c11Kids\"] =BES_census_data[\"c11Age0to4\"]+\\\n",
    "                            BES_census_data[\"c11Age5to7\"]+\\\n",
    "                            BES_census_data[\"c11Age8to9\"]+\\\n",
    "                            BES_census_data[\"c11Age10to14\"]+\\\n",
    "                            BES_census_data[\"c11Age15\"]+\\\n",
    "                            BES_census_data[\"c11Age16to17\"]\n",
    "# 'c11Age0to4', 'c11Age5to7', 'c11Age8to9', 'c11Age10to14', 'c11Age15',  'c11Age16to17',\n",
    "## young adults\n",
    "BES_census_data[\"c11YoungAdults\"] =BES_census_data[\"c11Age18to19\"]+\\\n",
    "                            BES_census_data[\"c11Age20to24\"]+\\\n",
    "                            BES_census_data[\"c11Age25to29\"]\n",
    "# 'c11Age18to19', 'c11Age20to24', 'c11Age25to29',\n",
    "## adult\n",
    "BES_census_data[\"c11Adults\"] =BES_census_data[\"c11Age30to44\"]+\\\n",
    "                              BES_census_data[\"c11Age45to59\"]\n",
    "# 'c11Age30to44', 'c11Age45to59',\n",
    "## elderly\n",
    "BES_census_data[\"c11Elderly\"] =BES_census_data[\"c11Age60to64\"]+\\\n",
    "                            BES_census_data[\"c11Age65to74\"]+\\\n",
    "                            BES_census_data[\"c11Age75to84\"]+\\\n",
    "                            BES_census_data[\"c11Age85to89\"]+\\\n",
    "                            BES_census_data[\"c11Age90plus\"]\n",
    "# 'c11Age60to64', 'c11Age65to74', 'c11Age75to84', 'c11Age85to89', 'c11Age90plus'\n",
    "BES_census_data[\"c11HealthMean\"] = (BES_census_data[\"c11HealthVeryBad\"]+\\\n",
    "                                    BES_census_data[\"c11HealthBad\"])-\\\n",
    "                                    (BES_census_data[\"c11HealthFair\"]+\\\n",
    "                                    BES_census_data[\"c11HealthGood\"])\n",
    "                                    ## still not very good\n",
    "BES_census_data[\"c11HealthVeryBad&Bad\"] = BES_census_data[\"c11HealthVeryBad\"]+BES_census_data[\"c11HealthBad\"]\n",
    "BES_census_data[\"c11HealthFair&Good\"] = BES_census_data[\"c11HealthFair\"]+BES_census_data[\"c11HealthGood\"]\n",
    "BES_census_data[\"c11CarsMoreThanNone\"] = BES_census_data[\"c11CarsOne\"]+BES_census_data[\"c11CarsTwo\"]+BES_census_data[\"c11CarsThree\"]+BES_census_data[\"c11CarsFour\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71b6d8ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(BES_small_data_files+\"CentreForTowns\"+os.sep+\"pcon-classification-csv.csv\")\n",
    "\n",
    "# https://www.centrefortowns.org/our-towns\n",
    "    \n",
    "# Type\tDefinition\tNumber\n",
    "# Villages (less than 5,000)\tPlaces with less than 5,000 residents\t5,568\n",
    "# Communities (5-10k)\tPlaces with between 5,000 and 10,000 residents\t567\n",
    "# Small towns (10k-30k)\tTowns with between 10,000 and 30,000 residents\t550\n",
    "# Medium towns (30k – 75k)\tTowns with between 30,000 and 75,000 residents\t242\n",
    "# Large towns (over 75k)\tTowns with over 75,000 residents\t102\n",
    "# Core Cities\tCore cities as defined by Pike et al (2016)\t12\n",
    "\n",
    "## doesn't seem so consisent - where is 'communities!'\n",
    "replace_dict = {\"Village or Smaller\":\"Village or smaller\",}\n",
    "df[\"classification\"] = df[\"classification\"].replace(replace_dict)\n",
    "centre_for_towns = df.pivot(index='constituency_code', columns='classification', values='percent_of_constituency').fillna(0)\n",
    "BES_census_data[centre_for_towns.columns] = centre_for_towns.loc[BES_census_data[\"ONSConstID\"]].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9470784b",
   "metadata": {},
   "outputs": [],
   "source": [
    "parl_con_aggs = pd.read_csv(BES_small_data_files+\"parliamentary constituency data\"+os.sep+\"Lots of aggregated parliamentary data.csv\")\n",
    "\n",
    "parl_con_aggs = parl_con_aggs.set_index(\"ONSConstID\").drop(['Unnamed: 0', 'pano',\n",
    " 'RESULTS2019_OTHER', 'RESULTS2019_Registered Voters', 'YOUGOVMRP_DEC_Con', 'YOUGOVMRP_DEC_Lab', 'YOUGOVMRP_DEC_LD',\n",
    "       'YOUGOVMRP_DEC_Brexit', 'YOUGOVMRP_DEC_Green', 'YOUGOVMRP_DEC_SNP',\n",
    "       'YOUGOVMRP_DEC_PC', 'YOUGOVMRP_DEC_Other', 'YOUGOVMRP_NOV_Con',\n",
    "       'YOUGOVMRP_NOV_Lab', 'YOUGOVMRP_NOV_LD', 'YOUGOVMRP_NOV_Brexit',\n",
    "       'YOUGOVMRP_NOV_Green', 'YOUGOVMRP_NOV_SNP', 'YOUGOVMRP_NOV_PC',\n",
    "       'YOUGOVMRP_NOV_Other', 'ConVote17', 'TotalVote17', 'RejectedVote17',\n",
    "       'Electorate17', 'ConVote15', 'TotalVote15', 'RejectedVote15',\n",
    "       'Electorate15', 'ConVote10', 'TotalVote10', 'Electorate10',\n",
    "       'c11Population','RESULTS2019_CON', 'RESULTS2019_LAB',\n",
    "       'RESULTS2019_LIBDEM', 'RESULTS2019_GRN', 'RESULTS2019_SNP',\n",
    "       'RESULTS2019_PC', 'RESULTS2019_BXP', 'RESULTS2019_UKIP',\n",
    "       'RESULTS2019_Turnout','Turn10'],axis=1).select_dtypes(['float64','int64'])\n",
    "parl_con_aggs = parl_con_aggs.drop(set(parl_con_aggs.columns).intersection(set(BES_census_data.columns)),axis=1)\n",
    "parl_con_aggs = parl_con_aggs.loc[BES_census_data[\"ONSConstID\"]]\n",
    "\n",
    "BES_census_data[parl_con_aggs.columns] = parl_con_aggs.values\n",
    "BES_census_data.columns = [x.replace(\"<\",\"LT\") for x in BES_census_data.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb45d0da",
   "metadata": {},
   "outputs": [],
   "source": [
    "BES_constituency_aggregates = pd.read_csv(BES_small_data_files+\"parliamentary constituency data\"+os.sep+\"BES_constituency_aggregates.csv\")\n",
    "BES_constituency_aggregates = BES_constituency_aggregates.set_index(\"ons_const_id\").drop([\"Unnamed: 0\",\"constituency_name\",\"pano\"],axis=1).loc[BES_census_data[\"ONSConstID\"]]\n",
    "# (632, 6868)\n",
    "\n",
    "## a lot - won't add before testing!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a64a25e",
   "metadata": {},
   "outputs": [],
   "source": [
    "nomis_dump = pd.read_csv(BES_data_folder+\"Nomis\"+os.sep+\"2011ONS_England&Wales_by_parlcon.csv\")\n",
    "nomis_dump = nomis_dump.set_index(\"Unnamed: 0\").loc[BES_census_data[\"ONSConstID\"][~BES_census_data[\"Country\"].isin([\"Scotland\"])]]\n",
    "## England&Wales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fee1ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "BES_census_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f432450e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BES_census_data.columns = [x.replace(\"<\",\"LT\") for x in BES_census_data.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05cf05c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "# set the filepath and load in a shapefile\n",
    "fp = BES_small_data_files+\"wpc.json\"\n",
    "map_df = gpd.read_file(fp)\n",
    "# check data type so we can see that this is not a normal dataframe, but a GEOdataframe\n",
    "# map_df.head()\n",
    "# now let's preview what our map looks like with no data in it\n",
    "# map_df.plot();\n",
    "# join the geodataframe with the cleaned up csv dataframe\n",
    "merged = map_df.set_index(\"PCON13CD\").join(BES_census_data.set_index(\"ONSConstID\"))\n",
    "merged = merged.loc[BES_census_data[\"ONSConstID\"]]\n",
    "merged.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3e06e56",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11331f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_cols = ['Winner17', 'Winner15', 'Winner10','pano',#category\n",
    "            'ConVote17', 'TotalVote17', 'Electorate17', 'ConVote15', 'TotalVote15',\n",
    "       'Electorate15', 'ConVote10', 'TotalVote10', 'Electorate10', #int\n",
    "             'LabVote19', 'Electorate19','RejectedVote19','LabVote05',\n",
    "              'Lab17', 'LD17', 'SNP17', 'PC17', 'UKIP17', 'Green17',\n",
    "             'Con0510', 'Lab0510', 'LD0510', 'SNP0510', 'PC0510',\n",
    "       'BNP0510', 'UKIP0510', 'Green0510',\n",
    "             'Con19', 'Lab19', 'LD19', 'SNP19', 'PC19', 'UKIP19', 'Green19',\n",
    "       'Brexit19', 'Other19', 'Majority19', 'Turnout19', 'TotalVote19',\n",
    "       'Con1719', 'Lab1719', 'LD1719', 'SNP1719', 'PC1719', 'UKIP1719',\n",
    "       'Green1719',\n",
    "             'ConVote19', 'LDVote19', 'SNPVote19', 'PCVote19', 'UKIPVote19',\n",
    "       'GreenVote19', 'BrexitVote19',\n",
    "             'leaveHanretty', 'remainHanretty',\n",
    "             'Con05', 'Lab05', 'LD05', 'SNP05', 'PC05', 'UKIP05',\n",
    "       'Green05', 'BNP05', 'Other05', 'Majority05', 'Turnout05', 'ConVote05',\n",
    "       'LDVote05', 'SNPVote05', 'PCVote05', 'UKIPVote05', 'GreenVote05',\n",
    "       'BNPVote05',\n",
    "             'Winner05', 'ConPPCsex19', 'LabPPCsex19',\n",
    "       'LDPPCsex19', 'SNPPPCsex19', 'PCPPCsex19', 'UKIPPPCsex19',\n",
    "       'GreenPPCsex19', 'BrexitPPCsex19',\n",
    "             # 'Con17',\n",
    "       'Other17', 'Majority17', 'Turnout17', 'LabVote17', 'LDVote17',\n",
    "       'SNPVote17', 'PCVote17', 'UKIPVote17', 'GreenVote17', 'Con1517',\n",
    "       'Lab1517', 'LD1517', 'SNP1517', 'PC1517', 'UKIP1517', 'Green1517',\n",
    "       'Con15', 'Lab15', 'LD15', 'SNP15', 'PC15', 'UKIP15', 'Green15',\n",
    "       'Other15', 'Majority15', 'Turnout15', 'LabVote15', 'LDVote15',\n",
    "       'SNPVote15', 'PCVote15', 'UKIPVote15', 'GreenVote15', 'BNPVote15',\n",
    "       'Con1015', 'Lab1015', 'LD1015', 'SNP1015', 'PC1015', 'UKIP1015',\n",
    "       'Green1015', 'Con10', 'Lab10', 'LD10', 'SNP10', 'PC10', 'UKIP10',\n",
    "       'Green10', 'BNP10', 'Majority10', 'LabVote10', 'LDVote10',\n",
    "       'SNPVote10', 'PCVote10', 'UKIPVote10', 'GreenVote10', 'BNPVote10',## float\n",
    "       'ONSConstID', 'ConstituencyName', 'SeatChange1517', 'SeatChange1015',\n",
    "       'ConPPC17', 'LabPPC17', 'LDPPC17', 'SNPPPC17', 'PCPPC17', 'UKIPPPC17',\n",
    "       'GreenPPC17', 'ConPPC15', 'LabPPC15', 'LDPPC15', 'SNPPPC15', 'PCPPC15',\n",
    "       'UKIPPPC15', 'GreenPPC15',     # object\n",
    "       'ONSConstID', 'ConstituencyName', 'SeatChange1719', 'SeatChange1517',\n",
    "       'SeatChange1015', 'ConPPC19', 'LabPPC19', 'LDPPC19', 'SNPPPC19',\n",
    "       'PCPPC19', 'UKIPPPC19', 'GreenPPC19', 'BrexitPPC19', 'ConPPC17',\n",
    "       'LabPPC17', 'LDPPC17', 'SNPPPC17', 'PCPPC17', 'UKIPPPC17', 'GreenPPC17',\n",
    "       'ConPPC15', 'LabPPC15', 'LDPPC15', 'SNPPPC15', 'PCPPC15', 'UKIPPPC15',\n",
    "       'GreenPPC15',\n",
    "             'ConLongSpendPercent',\n",
    "       'ConShortSpendPercent', 'LabLongSpendPercent', 'LabShortSpendPercent',\n",
    "       'LDLongSpendPercent', 'LDShortSpendPercent', 'SNPLongSpendPercent',\n",
    "       'SNPShortSpendPercent', 'PCLongSpendPercent', 'PCShortSpendPercent',\n",
    "       'UKIPLongSpendPercent', 'UKIPShortSpendPercent',\n",
    "       'GreenLongSpendPercent', 'GreenShortSpendPercent',\n",
    "             'FOCALDATAMRP_DEC_Conservative',\n",
    " 'FOCALDATAMRP_DEC_Labour',\n",
    " 'FOCALDATAMRP_DEC_Liberal Democrats',\n",
    " 'FOCALDATAMRP_DEC_SNP',\n",
    " 'FOCALDATAMRP_DEC_Other',\n",
    " 'FOCALDATAMRP_DEC_Green',\n",
    " 'FOCALDATAMRP_DEC_Brexit Party',\n",
    " 'FOCALDATAMRP_DEC_Plaid Cymru',\n",
    "        'Winner19',\n",
    "             'POLCOMP_Ec-Left-Right', 'POLCOMP_Soc-Lib-Con',\n",
    "             'Turnout10',\n",
    "            ]+['Con19_elec', 'Lab19_elec',\n",
    "       'LD19_elec', 'SNP19_elec', 'PC19_elec', 'UKIP19_elec', 'Green19_elec',\n",
    "       'Brexit19_elec', 'DNV19_elec', 'DNV&OTH19_elec', 'Auth_Right19_elec',\n",
    "       'Lib_Left19_elec', 'Con17_elec', 'Lab17_elec', 'LD17_elec',\n",
    "       'SNP17_elec', 'PC17_elec', 'UKIP17_elec', 'Green17_elec', 'DNV17_elec',\n",
    "       'DNV&OTH17_elec', 'Auth_Right17_elec', 'Lib_Left17_elec', 'Con15_elec',\n",
    "       'Lab15_elec', 'LD15_elec', 'SNP15_elec', 'PC15_elec', 'UKIP15_elec',\n",
    "       'Green15_elec', 'DNV15_elec', 'DNV&OTH15_elec', 'Auth_Right15_elec',\n",
    "       'Lib_Left15_elec', 'Con10_elec', 'Lab10_elec', 'LD10_elec',\n",
    "       'SNP10_elec', 'PC10_elec', 'UKIP10_elec', 'Green10_elec', 'BNP10_elec',\n",
    "       'DNV10_elec', 'DNV&OTH10_elec', 'Auth_Right10_elec', 'Lib_Left10_elec',\n",
    "       'Con05_elec', 'Lab05_elec', 'LD05_elec', 'SNP05_elec', 'PC05_elec',\n",
    "       'UKIP05_elec', 'Green05_elec', 'BNP05_elec', 'DNV05_elec',\n",
    "       'DNV&OTH05_elec', 'Auth_Right05_elec', 'Lib_Left05_elec']+['TotalVote19',\n",
    "                                                                  'TotalVote17', 'TotalVote15', 'TotalVote10',\n",
    "       'TotalVote05','CLOCKFACEREP_diversity', 'CLOCKFACEREP_security',\n",
    "       'CLOCKFACEREP_fecundity', 'CLOCKFACEREP_diversity_flatter',\n",
    "       'CLOCKFACEREP_security_flatter']\n",
    "\n",
    "# 'c11Population'\n",
    "\n",
    "# df_simp = pd.get_dummies(BES_census_data.drop(drop_cols,axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1f7dd58",
   "metadata": {},
   "outputs": [],
   "source": [
    "## add to drop cols, based on variable type X\n",
    "## decide how to setup merged/add variables to ... whichever dataset makes sense!  X\n",
    "## add all the constructed variables X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a74a2136",
   "metadata": {},
   "outputs": [],
   "source": [
    "## add cft data X\n",
    "## try BES_aggregates X\n",
    "## try parl const aggregates X\n",
    "## try nomis data X\n",
    "\n",
    "## quick xgboost breakdown of each/all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8538306",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "def dim_red(df,n_components=None,red_type=\"Factor Analysis\",show_first_x_comps=4):\n",
    "\n",
    "    Treatment = \"Census Data Decomposition\"\n",
    "    output_folder = \"E:\\\\BES_analysis_data\\\\output\\\\CensusDataDecomposition\\\\\"\n",
    "    output_subfolder = output_folder + Treatment + os.sep\n",
    "    if not os.path.exists( output_subfolder ):\n",
    "        os.makedirs( output_subfolder )\n",
    "\n",
    "    decomp_index = df.index\n",
    "    decomp_columns = df.columns\n",
    "    clean_feature_set_std = StandardScaler().fit_transform(df.values)\n",
    "    decomp_std = pd.DataFrame(   clean_feature_set_std,\n",
    "                                 columns = decomp_columns,\n",
    "                                 index   = decomp_index      )\n",
    "    if n_components is None:\n",
    "        n_components = decomp_std.shape[1]\n",
    "    \n",
    "    # n_components = 4\n",
    "    # (svd_solver='full', n_components='mle',whiten=True)\n",
    "    \n",
    "    if red_type == \"Factor Analysis\":\n",
    "        decomp = FactorAnalysis(svd_method = 'lapack',n_components = n_components) ## ~10s ,n_components=30 -> 1.5 hr\n",
    "    elif red_type == \"PCA\":\n",
    "        decomp = PCA(n_components = n_components,svd_solver='full')    \n",
    "    decomp_method = str(decomp).split(\"(\")[0] \n",
    "    X_r = decomp.fit_transform(decomp_std)\n",
    "    BES_decomp = pd.DataFrame(   X_r,\n",
    "                                 columns = range(0,n_components),\n",
    "                                 index   = decomp_index)\n",
    "\n",
    "#     save = True # False => Load\n",
    "#     if save & ( 'decomp' in globals() ): # SAVE    ##( 'decomp' not in globals() )\n",
    "#         decomp_method = str(decomp).split(\"(\")[0] \n",
    "#         subdir = output_subfolder + decomp_method\n",
    "#         fname = subdir+ os.sep + decomp_method\n",
    "    subdir = output_subfolder + decomp_method\n",
    "    (BES_decomp, comp_labels, comp_dict) = display_components(n_components, decomp,\n",
    "                                                              decomp_columns, BES_decomp, manifest=None, \n",
    "                                                              save_folder = subdir,  \n",
    "                                                              show_first_x_comps= show_first_x_comps, show_histogram = False)    \n",
    "    return (BES_decomp, comp_labels, comp_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "386adc29",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame(columns=[\"threshold\",\"repeats\",\"results\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f062376d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ed1fe34",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# start = time.time()\n",
    "# print(\"hello\")\n",
    "# end = time.time()\n",
    "# print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27342d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "geomask = (BES_census_data[\"Country\"]!=\"Scotland\")&(BES_census_data[\"Winner17\"]!=\"Speaker\")\n",
    "df_simp = pd.get_dummies(BES_census_data.drop(drop_cols+['Con17'],axis=1))[geomask]\n",
    "df_simp[var_stub] = BES_census_data[var_stub][geomask].values\n",
    "var_stub = \"Con17_elec\"\n",
    "\n",
    "# results_list = []\n",
    "# n_components=5\n",
    "for n_comp in [5,10]:\n",
    "    for corr_thresh in [0,.1,.2,.3,.4,.5,.6,.7]:\n",
    "        #,0.01,.02,.05,.1\n",
    "        for repeat in range(1):\n",
    "            start = time.time()\n",
    "\n",
    "#             Treatment = var_stub\n",
    "\n",
    "#             var_list = [var_stub]\n",
    "#             var_stub_list = [var_stub,]\n",
    "#             mask = df_simp[var_stub].notnull()\n",
    "#             wt_cols = []\n",
    "#             min_features = 30\n",
    "#             colname = var_stub\n",
    "#             dependence_plots = False\n",
    "\n",
    "#             alg = get_xgboost_alg(classification_problem=False)\n",
    "#             alg.random_state = randint(0, 1000)\n",
    "#             # alg.n_estimators=1000\n",
    "#             title = var_stub\n",
    "\n",
    "#             (explainer, shap_values, train_columns, train_index, alg,output_subfolder)=\\\n",
    "#                 xgboost_run(subdir=colname,dataset=df_simp[mask].drop(wt_cols,axis=1),\n",
    "#                         var_list=var_list,var_stub_list=var_stub_list,\n",
    "#                         use_specific_weights=None,\n",
    "#                         min_features = min(df_simp.shape[1]-1,min_features),verbosity=0,\n",
    "#                         skip_bar_plot=True,dependence_plots=dependence_plots,alg=alg,eval_metric=eval_metric,                    \n",
    "#                         title = title)\n",
    "\n",
    "#             dist = pd.DataFrame(shap_values).abs().mean(axis=0).sort_values(ascending=False)\n",
    "#             print( train_columns[dist[dist>=threshold].index].shape,df_simp.shape )\n",
    "\n",
    "#             df = df_simp[train_columns[dist[dist>=threshold].index]][geomask]\n",
    "#             df = df.fillna(df.mean())\n",
    "            corrs = df_simp.drop(var_stub,axis=1).corrwith(df_simp[var_stub])\n",
    "            df = df_simp[corrs[corrs.abs()>=corr_thresh].index]\n",
    "\n",
    "            (BES_decomp, comp_labels, comp_dict) = dim_red(df, n_components=n_comp,red_type=\"Factor Analysis\",show_first_x_comps=14)\n",
    "\n",
    "            Y = df_simp[var_stub]\n",
    "            X = BES_decomp.loc[:,0:n_comp]\n",
    "            X = sm.add_constant(X)\n",
    "            model = sm.OLS(Y,X)\n",
    "            results = model.fit()\n",
    "            display(results.summary())\n",
    "\n",
    "            ind = results_df.index.max()\n",
    "            if pd.isnull(ind):\n",
    "                ind = 0\n",
    "            else:\n",
    "                ind = ind+1\n",
    "            end = time.time()\n",
    "    #         print(end - start)            \n",
    "            results_df.loc[ind,[\"duration\",\"n_components\",\"variables\",\"corr_thresh\",\"repeats\",\"results\"]] = [end-start,n_comp,\n",
    "                                                                                                           df.columns,\n",
    "                                                                                                    corr_thresh,repeat,results]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a81c6e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a8a7c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df[\"rsquared\"] = results_df[\"results\"].apply(lambda x: x.rsquared)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f6dc59",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.groupby(['threshold','n_components']).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf54fe00",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.groupby(['threshold','n_components'])['repeats'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80bd8de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "var_stub = \"Con17_elec\"\n",
    "geomask = (BES_census_data[\"Country\"]!=\"Scotland\")&(BES_census_data[\"Winner17\"]!=\"Speaker\")\n",
    "df_simp = pd.get_dummies(BES_census_data.drop(drop_cols+['Con17'],axis=1))[geomask]\n",
    "df_simp[var_stub] = BES_census_data[var_stub][geomask].values\n",
    "\n",
    "\n",
    "# results_list = []\n",
    "# n_components=5\n",
    "for n_comp in [6,7,8,9]:\n",
    "    for threshold in [0.0,0.005,.01,.02,.05,.1,.2]:\n",
    "        #,0.01,.02,.05,.1\n",
    "        for repeat in range(20):\n",
    "            start = time.time()\n",
    "\n",
    "            Treatment = var_stub\n",
    "\n",
    "            var_list = [var_stub]\n",
    "            var_stub_list = [var_stub,]\n",
    "            mask = df_simp[var_stub].notnull()\n",
    "            wt_cols = []\n",
    "            min_features = 30\n",
    "            colname = var_stub\n",
    "            dependence_plots = False\n",
    "\n",
    "            alg = get_xgboost_alg(classification_problem=False)\n",
    "            alg.random_state = randint(0, 1000)\n",
    "            # alg.n_estimators=1000\n",
    "            title = var_stub\n",
    "\n",
    "            (explainer, shap_values, train_columns, train_index, alg,output_subfolder)=\\\n",
    "                xgboost_run(subdir=colname,dataset=df_simp[mask].drop(wt_cols,axis=1),\n",
    "                        var_list=var_list,var_stub_list=var_stub_list,\n",
    "                        use_specific_weights=None,\n",
    "                        min_features = min(df_simp.shape[1]-1,min_features),verbosity=0,\n",
    "                        skip_bar_plot=True,dependence_plots=dependence_plots,alg=alg,eval_metric=eval_metric,                    \n",
    "                        title = title)\n",
    "\n",
    "            dist = pd.DataFrame(shap_values).abs().mean(axis=0).sort_values(ascending=False)\n",
    "            print( train_columns[dist[dist>=threshold].index].shape,df_simp.shape )\n",
    "\n",
    "            df = df_simp[train_columns[dist[dist>=threshold].index]][geomask]\n",
    "            df = df.fillna(df.mean())\n",
    "\n",
    "            (BES_decomp, comp_labels, comp_dict) = dim_red(df, n_components=n_comp,red_type=\"Factor Analysis\",show_first_x_comps=14)\n",
    "\n",
    "            Y = df_simp[var_stub]\n",
    "            X = BES_decomp.loc[:,0:n_comp]\n",
    "            X = sm.add_constant(X)\n",
    "            model = sm.OLS(Y,X)\n",
    "            results = model.fit()\n",
    "            display(results.summary())\n",
    "\n",
    "            ind = results_df.index.max()\n",
    "            if pd.isnull(ind):\n",
    "                ind = 0\n",
    "            else:\n",
    "                ind = ind+1\n",
    "            end = time.time()\n",
    "    #         print(end - start)            \n",
    "            results_df.loc[ind,[\"duration\",\"n_components\",\"variables\",\"threshold\",\"repeats\",\"results\"]] = [end-start,n_comp,\n",
    "                                                                                                           train_columns[dist[dist>=threshold].index],\n",
    "                                                                                                    threshold,repeat,results]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "329166e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62085a90",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c446659",
   "metadata": {},
   "outputs": [],
   "source": [
    "# results_df[\"n_components\"] = results_df[\"n_components\"].fillna(10)\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fa58da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.groupby(['threshold','n_components']).std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad07ceec",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc614eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# results_df[\"variables\"].apply(lambda x: len(x)).groupby(results_df[['threshold','n_components']]).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4866efa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_simp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "298a5e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## add timing\n",
    "# 5 mins per round\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38aa67da",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_simp = BES_constituency_aggregates.drop(['pano','constituency_name','Con17'],axis=1)\\\n",
    "    .drop(search(BES_constituency_aggregates,\"|\".join([\"winConstituency\",\"profile_past\",\"ashcroft\",\"partyContact\",\n",
    "                                                       \"partyId\",\"localElection\",\"localTurnout\",\"^map\",\n",
    "                                                       \"generalElectionVote\",\"euroElectionVote\",\n",
    "                                                       \"^prefer[^M]\",\"ptv\"])).index,axis=1)\n",
    "df_simp[\"Con17_elec\"] = BES_census_data[\"Con17_elec\"].values\n",
    "\n",
    "var_stub = \"Con17_elec\"\n",
    "Treatment = var_stub\n",
    "\n",
    "var_list = [var_stub]\n",
    "var_stub_list = [var_stub,]\n",
    "mask = df_simp[var_stub].notnull()\n",
    "wt_cols = []\n",
    "min_features = 30\n",
    "colname = \"Con17_elec\"\n",
    "dependence_plots = False\n",
    "\n",
    "alg = get_xgboost_alg(classification_problem=False)\n",
    "title = \"Con17_elec\"\n",
    "\n",
    "(explainer, shap_values, train_columns, train_index, alg,output_subfolder)=\\\n",
    "    xgboost_run(subdir=colname,dataset=df_simp[mask].drop(wt_cols,axis=1),\n",
    "            var_list=var_list,var_stub_list=var_stub_list,\n",
    "            use_specific_weights=None,\n",
    "            min_features = min(df_simp.shape[1]-1,min_features),verbosity=0,\n",
    "            skip_bar_plot=True,dependence_plots=dependence_plots,alg=alg,eval_metric=eval_metric,                    \n",
    "            title = title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4436a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "dist = pd.DataFrame(shap_values).abs().mean(axis=0).sort_values(ascending=False)\n",
    "train_columns[dist[dist>=.1].index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd627029",
   "metadata": {},
   "outputs": [],
   "source": [
    "# less shit than expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82f77c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_simp = BES_census_data[[\"Con17\"]]\n",
    "df_simp.loc[BES_census_data[\"Country\"]!=\"Scotland\",nomis_dump.columns] = nomis_dump.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee0e323b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_simp = nomis_dump\n",
    "# df_simp[\"Con17\"] = BES_census_data[\"Con17\"].values\n",
    "\n",
    "var_stub = \"Con17\"\n",
    "Treatment = var_stub\n",
    "\n",
    "var_list = [var_stub]\n",
    "var_stub_list = [var_stub,]\n",
    "mask = df_simp[var_stub].notnull()\n",
    "wt_cols = []\n",
    "min_features = 30\n",
    "colname = \"Con17\"\n",
    "dependence_plots = False\n",
    "\n",
    "alg = get_xgboost_alg(classification_problem=False)\n",
    "title = \"Con17\"\n",
    "\n",
    "(explainer, shap_values, train_columns, train_index, alg,output_subfolder)=\\\n",
    "    xgboost_run(subdir=colname,dataset=df_simp[mask].drop(wt_cols,axis=1),\n",
    "            var_list=var_list,var_stub_list=var_stub_list,\n",
    "            use_specific_weights=None,\n",
    "            min_features = min(df_simp.shape[1]-1,min_features),verbosity=0,\n",
    "            skip_bar_plot=True,dependence_plots=dependence_plots,alg=alg,eval_metric=eval_metric,                    \n",
    "            title = title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e42efd4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_simp = BES_census_data[[\"Con17\"]]\n",
    "df_simp.loc[BES_census_data[\"Country\"]!=\"Scotland\",nomis_dump.columns] = nomis_dump.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7b46e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_simp = nomis_dump\n",
    "# df_simp[\"Con17\"] = BES_census_data[\"Con17\"].values\n",
    "\n",
    "var_stub = \"Con17\"\n",
    "Treatment = var_stub\n",
    "\n",
    "var_list = [var_stub]\n",
    "var_stub_list = [var_stub,]\n",
    "mask = df_simp[var_stub].notnull()\n",
    "wt_cols = []\n",
    "min_features = 30\n",
    "colname = \"Con17\"\n",
    "dependence_plots = False\n",
    "\n",
    "alg = get_xgboost_alg(classification_problem=False)\n",
    "title = \"Con17\"\n",
    "\n",
    "(explainer, shap_values, train_columns, train_index, alg,output_subfolder)=\\\n",
    "    xgboost_run(subdir=colname,dataset=df_simp[mask].drop(wt_cols,axis=1),\n",
    "            var_list=var_list,var_stub_list=var_stub_list,\n",
    "            use_specific_weights=None,\n",
    "            min_features = min(df_simp.shape[1]-1,min_features),verbosity=0,\n",
    "            skip_bar_plot=True,dependence_plots=dependence_plots,alg=alg,eval_metric=eval_metric,                    \n",
    "            title = title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9125d6d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "geomask = (BES_census_data[\"Country\"]!=\"Scotland\")&(BES_census_data[\"Winner17\"]!=\"Speaker\")\n",
    "df_simp = BES_census_data[[\"Con17\"]]\n",
    "df_simp.loc[BES_census_data[\"Country\"]!=\"Scotland\",nomis_dump.columns] = nomis_dump.values\n",
    "df_simp = df_simp.drop(\"Con17\",axis=1)\n",
    "df_simp = pd.get_dummies(pd.concat([df_simp,\n",
    "                     BES_constituency_aggregates.drop(['pano','constituency_name','Con17'],axis=1).reset_index().drop(\"ons_const_id\",axis=1),\n",
    "                     BES_census_data],axis=1)[geomask].drop(drop_cols,axis=1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c47afb9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_simp.shape,df_simp[\"Con17\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba2c8fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "var_stub = \"Con17\"\n",
    "Treatment = var_stub\n",
    "\n",
    "var_list = [var_stub]\n",
    "var_stub_list = [var_stub,]\n",
    "mask = df_simp[var_stub].notnull()\n",
    "wt_cols = []\n",
    "min_features = 30\n",
    "colname = \"Con17\"\n",
    "dependence_plots = False\n",
    "\n",
    "alg = get_xgboost_alg(classification_problem=False)\n",
    "title = \"Con17\"\n",
    "\n",
    "(explainer, shap_values, train_columns, train_index, alg,output_subfolder)=\\\n",
    "    xgboost_run(subdir=colname,dataset=df_simp[mask].drop(wt_cols,axis=1),\n",
    "            var_list=var_list,var_stub_list=var_stub_list,\n",
    "            use_specific_weights=None,\n",
    "            min_features = min(df_simp.shape[1]-1,min_features),verbosity=0,\n",
    "            skip_bar_plot=True,dependence_plots=dependence_plots,alg=alg,eval_metric=eval_metric,                    \n",
    "            title = title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5866ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = merged[merged[\"Country\"].isin([\"England\",\"Wales\"])]\n",
    "\n",
    "Y = df_simp['Con17']\n",
    "X = df_simp.fillna(df_simp.mean()).drop('Con17',axis=1)\n",
    "X = sm.add_constant(X)\n",
    "model = sm.OLS(Y,X)\n",
    "results = model.fit_regularized(method = 'sqrt_lasso',refit=True,zero_tol =.005)\n",
    "results.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad1cee66",
   "metadata": {},
   "outputs": [],
   "source": [
    "results.params.sort_values().head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "422ce3f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "results.params.sort_values().tail(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e69e31cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# - Con/Lab/LD/Lib-Left vs Auth-Right\n",
    "# - Include non-voting\n",
    "# - Do 19/17/15/10/05 X\n",
    "# - Zoom into areas X\n",
    "# - Think about feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14dedcc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "BES_census_data.columns[0:20]\n",
    "\n",
    "# Con,UKIP,Brexit\n",
    "# Lab,LD,SNP,PC,Green\n",
    "# Other - should turnout include this or not???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b4459c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9770827",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BES_census_data.drop(BES_census_data.columns[-82:],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0787d3d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "BES_census_data.columns[-50:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b207fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "var_stub = \"Con17\"\n",
    "title = \"Con17_jk_vars\"\n",
    "df_simp = pd.get_dummies(BES_census_data.drop(drop_cols,axis=1))\n",
    "specific_vars = [\"c11Deprived3\",\"c11QualLevel4\",\"c11Age60to64\",\"CON%MAN_DIR_SEN\",\"CON%SAL_SERV\",\n",
    "                 \"Merseyside\",\n",
    "                 \"speakWelsh\",\n",
    "                ]\n",
    "specific_vars = df_simp.columns\n",
    "window = BES_census_data[\"Region\"].isin(['North East', 'North West', 'Yorkshire and The Humber',\n",
    "#                                          'East Midlands','West Midlands'\n",
    "                                        ])\n",
    "window = None\n",
    "\n",
    "geomask = (BES_census_data[\"Country\"]!=\"Scotland\")&(BES_census_data[\"Winner17\"]!=\"Speaker\")\n",
    "\n",
    "sm_and_xgb_plot(var_stub,title,specific_vars,df_simp=df_simp,geomask=geomask,window=window)\n",
    "# sm_and_xgb_plot(var_stub,title,specific_vars,df_simp,geomask=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2477ad70",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "var_stub = \"Con17\"\n",
    "title = \"Con17_kitchen_sink\"\n",
    "df_simp = pd.get_dummies(BES_census_data.drop(drop_cols,axis=1)).drop(\"Con17\",axis=1)\n",
    "# specific_vars = [\"c11Deprived3\",\"c11QualLevel4\",\"c11Age60to64\",\"CON%MAN_DIR_SEN\",\"CON%SAL_SERV\",\n",
    "#                  \"Merseyside\",\n",
    "#                  \"speakWelsh\",\n",
    "#                 ]\n",
    "specific_vars = list(df_simp.columns)\n",
    "# window = BES_census_data[\"Region\"].isin(['North East', 'North West', 'Yorkshire and The Humber',\n",
    "#                                          'East Midlands','West Midlands'\n",
    "#                                         ])\n",
    "window = None\n",
    "\n",
    "geomask = (BES_census_data[\"Country\"]!=\"Scotland\")&(BES_census_data[\"Winner17\"]!=\"Speaker\")\n",
    "\n",
    "sm_and_xgb_plot(var_stub,title,specific_vars,df_simp=df_simp,geomask=geomask,window=window)\n",
    "# sm_and_xgb_plot(var_stub,title,specific_vars,df_simp,geomask=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e7044ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "var_stub = \"Lab17\"\n",
    "title = \"Lab17_kitchen_sink\"\n",
    "df_simp = pd.get_dummies(BES_census_data.drop(drop_cols,axis=1)).drop(\"Con17\",axis=1)\n",
    "specific_vars = list(df_simp.columns)\n",
    "window = None\n",
    "geomask = (BES_census_data[\"Country\"]!=\"Scotland\")&(BES_census_data[\"Winner17\"]!=\"Speaker\")\n",
    "sm_and_xgb_plot(var_stub,title,specific_vars,df_simp=df_simp,geomask=geomask,window=window)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25e9b6c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "var_stub = \"Lab15\"\n",
    "title = \"Lab15_kitchen_sink\"\n",
    "df_simp = pd.get_dummies(BES_census_data.drop(drop_cols,axis=1)).drop(\"Con17\",axis=1)\n",
    "specific_vars = list(df_simp.columns)\n",
    "window = None\n",
    "geomask = (BES_census_data[\"Country\"]!=\"Scotland\")&(BES_census_data[\"Winner15\"]!=\"Speaker\")\n",
    "sm_and_xgb_plot(var_stub,title,specific_vars,df_simp=df_simp,geomask=geomask,window=window)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c6e61ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "var_stub = \"Lab10\"\n",
    "title = \"Lab10_kitchen_sink\"\n",
    "df_simp = pd.get_dummies(BES_census_data.drop(drop_cols,axis=1)).drop(\"Con17\",axis=1)\n",
    "specific_vars = list(df_simp.columns)\n",
    "window = None\n",
    "geomask = (BES_census_data[\"Country\"]!=\"Scotland\")&(BES_census_data[\"Winner10\"]!=\"Speaker\")\n",
    "sm_and_xgb_plot(var_stub,title,specific_vars,df_simp=df_simp,geomask=geomask,window=window)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b775a420",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "var_stub = \"Lab05\"\n",
    "title = \"Lab05_kitchen_sink\"\n",
    "df_simp = pd.get_dummies(BES_census_data.drop(drop_cols,axis=1)).drop(\"Con17\",axis=1)\n",
    "specific_vars = list(df_simp.columns)\n",
    "window = None\n",
    "geomask = (BES_census_data[\"Country\"]!=\"Scotland\")&(BES_census_data[\"Winner05\"]!=\"Speaker\")\n",
    "sm_and_xgb_plot(var_stub,title,specific_vars,df_simp=df_simp,geomask=geomask,window=window)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3b343b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "BES_census_data[\"Winner19\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10d75e9f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bc9aadb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "var_stub = \"LD17\"\n",
    "title = \"LD17_kitchen_sink\"\n",
    "df_simp = pd.get_dummies(BES_census_data.drop(drop_cols,axis=1)).drop(\"Con17\",axis=1)\n",
    "# specific_vars = [\"c11Deprived3\",\"c11QualLevel4\",\"c11Age60to64\",\"CON%MAN_DIR_SEN\",\"CON%SAL_SERV\",\n",
    "#                  \"Merseyside\",\n",
    "#                  \"speakWelsh\",\n",
    "#                 ]\n",
    "specific_vars = list(df_simp.columns)\n",
    "# window = BES_census_data[\"Region\"].isin(['North East', 'North West', 'Yorkshire and The Humber',\n",
    "#                                          'East Midlands','West Midlands'\n",
    "#                                         ])\n",
    "window = None\n",
    "\n",
    "geomask = (BES_census_data[\"Country\"]!=\"Scotland\")&(BES_census_data[\"Winner17\"]!=\"Speaker\")\n",
    "\n",
    "sm_and_xgb_plot(var_stub,title,specific_vars,df_simp=df_simp,geomask=geomask,window=window)\n",
    "# sm_and_xgb_plot(var_stub,title,specific_vars,df_simp,geomask=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6475edf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "var_stub = \"LD15\"\n",
    "title = \"LD15_kitchen_sink\"\n",
    "df_simp = pd.get_dummies(BES_census_data.drop(drop_cols,axis=1)).drop(\"Con17\",axis=1)\n",
    "specific_vars = list(df_simp.columns)\n",
    "window = None\n",
    "geomask = (BES_census_data[\"Country\"]!=\"Scotland\")&(BES_census_data[\"Winner15\"]!=\"Speaker\")\n",
    "sm_and_xgb_plot(var_stub,title,specific_vars,df_simp=df_simp,geomask=geomask,window=window)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdcf6047",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "var_stub = \"LD10\"\n",
    "title = \"LD10_kitchen_sink\"\n",
    "df_simp = pd.get_dummies(BES_census_data.drop(drop_cols,axis=1)).drop(\"Con17\",axis=1)\n",
    "specific_vars = list(df_simp.columns)\n",
    "window = None\n",
    "geomask = (BES_census_data[\"Country\"]!=\"Scotland\")&(BES_census_data[\"Winner10\"]!=\"Speaker\")\n",
    "sm_and_xgb_plot(var_stub,title,specific_vars,df_simp=df_simp,geomask=geomask,window=window)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38d8556d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "var_stub = \"LD05\"\n",
    "title = \"LD05_kitchen_sink\"\n",
    "df_simp = pd.get_dummies(BES_census_data.drop(drop_cols,axis=1)).drop(\"Con17\",axis=1)\n",
    "specific_vars = list(df_simp.columns)\n",
    "window = None\n",
    "geomask = (BES_census_data[\"Country\"]!=\"Scotland\")&(BES_census_data[\"Winner05\"]!=\"Speaker\")\n",
    "sm_and_xgb_plot(var_stub,title,specific_vars,df_simp=df_simp,geomask=geomask,window=window)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "869a4c80",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "291169a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90a502c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "var_stub = \"UKIP17\"\n",
    "title = \"UKIP17_kitchen_sink\"\n",
    "df_simp = pd.get_dummies(BES_census_data.drop(drop_cols,axis=1)).drop(\"Con17\",axis=1)\n",
    "# specific_vars = [\"c11Deprived3\",\"c11QualLevel4\",\"c11Age60to64\",\"CON%MAN_DIR_SEN\",\"CON%SAL_SERV\",\n",
    "#                  \"Merseyside\",\n",
    "#                  \"speakWelsh\",\n",
    "#                 ]\n",
    "specific_vars = list(df_simp.columns)\n",
    "# window = BES_census_data[\"Region\"].isin(['North East', 'North West', 'Yorkshire and The Humber',\n",
    "#                                          'East Midlands','West Midlands'\n",
    "#                                         ])\n",
    "window = None\n",
    "\n",
    "geomask = (BES_census_data[\"Country\"]!=\"Scotland\")&(BES_census_data[\"Winner17\"]!=\"Speaker\")\n",
    "\n",
    "sm_and_xgb_plot(var_stub,title,specific_vars,df_simp=df_simp,geomask=geomask,window=window)\n",
    "# sm_and_xgb_plot(var_stub,title,specific_vars,df_simp,geomask=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "495ef3cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "ge = \"15\"\n",
    "var_stub = \"UKIP\"+ge\n",
    "title = \"UKIP\"+ge+\"_kitchen_sink\"\n",
    "df_simp = pd.get_dummies(BES_census_data.drop(drop_cols,axis=1)).drop(\"Con17\",axis=1)\n",
    "specific_vars = list(df_simp.columns)\n",
    "window = None\n",
    "geomask = (BES_census_data[\"Country\"]!=\"Scotland\")&(BES_census_data[\"Winner\"+ge]!=\"Speaker\")\n",
    "sm_and_xgb_plot(var_stub,title,specific_vars,df_simp=df_simp,geomask=geomask,window=window)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7586780b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "ge = \"10\"\n",
    "var_stub = \"UKIP\"+ge\n",
    "title = \"UKIP\"+ge+\"_kitchen_sink\"\n",
    "df_simp = pd.get_dummies(BES_census_data.drop(drop_cols,axis=1)).drop(\"Con17\",axis=1)\n",
    "specific_vars = list(df_simp.columns)\n",
    "window = None\n",
    "geomask = (BES_census_data[\"Country\"]!=\"Scotland\")&(BES_census_data[\"Winner\"+ge]!=\"Speaker\")\n",
    "sm_and_xgb_plot(var_stub,title,specific_vars,df_simp=df_simp,geomask=geomask,window=window)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dc2ead3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "ge = \"05\"\n",
    "var_stub = \"UKIP\"+ge\n",
    "title = \"UKIP\"+ge+\"_kitchen_sink\"\n",
    "df_simp = pd.get_dummies(BES_census_data.drop(drop_cols,axis=1)).drop(\"Con17\",axis=1)\n",
    "specific_vars = list(df_simp.columns)\n",
    "window = None\n",
    "geomask = (BES_census_data[\"Country\"]!=\"Scotland\")&(BES_census_data[\"Winner\"+ge]!=\"Speaker\")\n",
    "sm_and_xgb_plot(var_stub,title,specific_vars,df_simp=df_simp,geomask=geomask,window=window)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7446ae01",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "var_stub = \"Con15\"\n",
    "title = \"Con15_kitchen_sink\"\n",
    "df_simp = pd.get_dummies(BES_census_data.drop(drop_cols,axis=1)).drop(\"Con17\",axis=1)\n",
    "# specific_vars = [\"c11Deprived3\",\"c11QualLevel4\",\"c11Age60to64\",\"CON%MAN_DIR_SEN\",\"CON%SAL_SERV\",\n",
    "#                  \"Merseyside\",\n",
    "#                  \"speakWelsh\",\n",
    "#                 ]\n",
    "specific_vars = list(df_simp.columns)\n",
    "# window = BES_census_data[\"Region\"].isin(['North East', 'North West', 'Yorkshire and The Humber',\n",
    "#                                          'East Midlands','West Midlands'\n",
    "#                                         ])\n",
    "window = None\n",
    "\n",
    "geomask = (BES_census_data[\"Country\"]!=\"Scotland\")&(BES_census_data[\"Winner17\"]!=\"Speaker\")\n",
    "\n",
    "sm_and_xgb_plot(var_stub,title,specific_vars,df_simp=df_simp,geomask=geomask,window=window)\n",
    "# sm_and_xgb_plot(var_stub,title,specific_vars,df_simp,geomask=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15bd6a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "var_stub = \"Con10\"\n",
    "title = \"Con10_kitchen_sink\"\n",
    "df_simp = pd.get_dummies(BES_census_data.drop(drop_cols,axis=1)).drop(\"Con17\",axis=1)\n",
    "# specific_vars = [\"c11Deprived3\",\"c11QualLevel4\",\"c11Age60to64\",\"CON%MAN_DIR_SEN\",\"CON%SAL_SERV\",\n",
    "#                  \"Merseyside\",\n",
    "#                  \"speakWelsh\",\n",
    "#                 ]\n",
    "specific_vars = list(df_simp.columns)\n",
    "# window = BES_census_data[\"Region\"].isin(['North East', 'North West', 'Yorkshire and The Humber',\n",
    "#                                          'East Midlands','West Midlands'\n",
    "#                                         ])\n",
    "window = None\n",
    "\n",
    "geomask = (BES_census_data[\"Country\"]!=\"Scotland\")&(BES_census_data[\"Winner17\"]!=\"Speaker\")\n",
    "\n",
    "sm_and_xgb_plot(var_stub,title,specific_vars,df_simp=df_simp,geomask=geomask,window=window)\n",
    "# sm_and_xgb_plot(var_stub,title,specific_vars,df_simp,geomask=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4ae4f27",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "var_stub = \"Con05\"\n",
    "title = \"Con05_kitchen_sink\"\n",
    "df_simp = pd.get_dummies(BES_census_data.drop(drop_cols,axis=1)).drop(\"Con17\",axis=1)\n",
    "# specific_vars = [\"c11Deprived3\",\"c11QualLevel4\",\"c11Age60to64\",\"CON%MAN_DIR_SEN\",\"CON%SAL_SERV\",\n",
    "#                  \"Merseyside\",\n",
    "#                  \"speakWelsh\",\n",
    "#                 ]\n",
    "specific_vars = list(df_simp.columns)\n",
    "# window = BES_census_data[\"Region\"].isin(['North East', 'North West', 'Yorkshire and The Humber',\n",
    "#                                          'East Midlands','West Midlands'\n",
    "#                                         ])\n",
    "window = None\n",
    "\n",
    "geomask = (BES_census_data[\"Country\"]!=\"Scotland\")&(BES_census_data[\"Winner17\"]!=\"Speaker\")\n",
    "\n",
    "sm_and_xgb_plot(var_stub,title,specific_vars,df_simp=df_simp,geomask=geomask,window=window)\n",
    "# sm_and_xgb_plot(var_stub,title,specific_vars,df_simp,geomask=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbb142b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7d281c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "ge = \"17\"\n",
    "var_stub = \"Con\"+ge+\"_elec\"\n",
    "title = \"Con\"+ge+\"_elec_kitchen_sink\"\n",
    "df_simp = pd.get_dummies(BES_census_data.drop(drop_cols,axis=1)).drop(\"Con17\",axis=1)\n",
    "specific_vars = list(df_simp.columns)\n",
    "window = None\n",
    "geomask = (BES_census_data[\"Country\"]!=\"Scotland\")&(BES_census_data[\"Winner\"+ge]!=\"Speaker\")\n",
    "sm_and_xgb_plot(var_stub,title,specific_vars,df_simp=df_simp,geomask=geomask,window=window)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abde78be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "ge = \"17\"\n",
    "var_stub = \"Lab\"+ge+\"_elec\"\n",
    "title = \"Lab\"+ge+\"_elec_kitchen_sink\"\n",
    "df_simp = pd.get_dummies(BES_census_data.drop(drop_cols,axis=1)).drop(\"Con17\",axis=1)\n",
    "specific_vars = list(df_simp.columns)\n",
    "window = None\n",
    "geomask = (BES_census_data[\"Country\"]!=\"Scotland\")&(BES_census_data[\"Winner\"+ge]!=\"Speaker\")\n",
    "sm_and_xgb_plot(var_stub,title,specific_vars,df_simp=df_simp,geomask=geomask,window=window)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f776ef79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "ge = \"17\"\n",
    "var_stub = \"DNV\"+ge+\"_elec\"\n",
    "title = \"DNV\"+ge+\"_elec_kitchen_sink\"\n",
    "df_simp = pd.get_dummies(BES_census_data.drop(drop_cols,axis=1)).drop(\"Con17\",axis=1)\n",
    "specific_vars = list(df_simp.columns)\n",
    "window = None\n",
    "geomask = (BES_census_data[\"Country\"]!=\"Scotland\")&(BES_census_data[\"Winner\"+ge]!=\"Speaker\")\n",
    "sm_and_xgb_plot(var_stub,title,specific_vars,df_simp=df_simp,geomask=geomask,window=window)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cfb2a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%timeDNV&OTH17_elec\n",
    "# nope -> worse!\n",
    "ge = \"17\"\n",
    "var_stub = \"DNV&OTH\"+ge+\"_elec\"\n",
    "title = \"DNV&OTH\"+ge+\"_elec_kitchen_sink\"\n",
    "df_simp = pd.get_dummies(BES_census_data.drop(drop_cols,axis=1)).drop(\"Con17\",axis=1)\n",
    "specific_vars = list(df_simp.columns)\n",
    "window = None\n",
    "geomask = (BES_census_data[\"Country\"]!=\"Scotland\")&(BES_census_data[\"Winner\"+ge]!=\"Speaker\")\n",
    "sm_and_xgb_plot(var_stub,title,specific_vars,df_simp=df_simp,geomask=geomask,window=window)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0131fc76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%timeAuth_Right17_elec\n",
    "ge = \"17\"\n",
    "var_stub = \"Auth_Right\"+ge+\"_elec\"\n",
    "title = \"Auth_Right\"+ge+\"_elec_kitchen_sink\"\n",
    "df_simp = pd.get_dummies(BES_census_data.drop(drop_cols,axis=1)).drop(\"Con17\",axis=1)\n",
    "specific_vars = list(df_simp.columns)\n",
    "window = None\n",
    "geomask = (BES_census_data[\"Country\"]!=\"Scotland\")&(BES_census_data[\"Winner\"+ge]!=\"Speaker\")\n",
    "sm_and_xgb_plot(var_stub,title,specific_vars,df_simp=df_simp,geomask=geomask,window=window)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dbd75d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%timeAuth_Right17_elec\n",
    "ge = \"17\"\n",
    "var_stub = \"Con\"+ge+\"_elec\"\n",
    "title = \"Con\"+ge+\"_elec_kitchen_sink_england\"\n",
    "df_simp = pd.get_dummies(BES_census_data.drop(drop_cols,axis=1)).drop(\"Con17\",axis=1)\n",
    "specific_vars = list(df_simp.columns)\n",
    "window = None\n",
    "geomask = (BES_census_data[\"Country\"]==\"England\")&(BES_census_data[\"Winner\"+ge]!=\"Speaker\")\n",
    "sm_and_xgb_plot(var_stub,title,specific_vars,df_simp=df_simp,geomask=geomask,window=window)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1ba1297",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%timeAuth_Right17_elec\n",
    "ge = \"17\"\n",
    "var_stub = \"Auth_Right\"+ge+\"_elec\"\n",
    "title = \"Auth_Right\"+ge+\"_elec_kitchen_sink_england\"\n",
    "df_simp = pd.get_dummies(BES_census_data.drop(drop_cols,axis=1)).drop(\"Con17\",axis=1)\n",
    "specific_vars = list(df_simp.columns)\n",
    "window = None\n",
    "geomask = (BES_census_data[\"Country\"]==\"England\")&(BES_census_data[\"Winner\"+ge]!=\"Speaker\")\n",
    "sm_and_xgb_plot(var_stub,title,specific_vars,df_simp=df_simp,geomask=geomask,window=window)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfabe8fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%timeAuth_Right17_elec\n",
    "ge = \"17\"\n",
    "var_stub = \"Lib_Left\"+ge+\"_elec\"\n",
    "title = \"Lib_Left\"+ge+\"_elec_kitchen_sink\"\n",
    "df_simp = pd.get_dummies(BES_census_data.drop(drop_cols,axis=1)).drop(\"Con17\",axis=1)\n",
    "specific_vars = list(df_simp.columns)\n",
    "window = None\n",
    "geomask = (BES_census_data[\"Country\"]==\"England\")&(BES_census_data[\"Winner\"+ge]!=\"Speaker\")\n",
    "sm_and_xgb_plot(var_stub,title,specific_vars,df_simp=df_simp,geomask=geomask,window=window)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f52b09e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%timeAuth_Right17_elec\n",
    "ge = \"17\"\n",
    "var_stub = \"Lab\"+ge+\"_elec\"\n",
    "title = \"Lab\"+ge+\"_elec_kitchen_sink\"\n",
    "df_simp = pd.get_dummies(BES_census_data.drop(drop_cols,axis=1)).drop(\"Con17\",axis=1)\n",
    "specific_vars = list(df_simp.columns)\n",
    "window = None\n",
    "geomask = (BES_census_data[\"Country\"]==\"England\")&(BES_census_data[\"Winner\"+ge]!=\"Speaker\")\n",
    "sm_and_xgb_plot(var_stub,title,specific_vars,df_simp=df_simp,geomask=geomask,window=window)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fcac7b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## confusing pattern with Lib_left/Lab r^2 in England&Wales/England\n",
    "## opposite patternw with Right_auth/Con r^ in England&Wales/England"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a634fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO\n",
    "## option to suppress sm/geo plots for easy topline stuff\n",
    "## feature selection\n",
    "## -> try to get top X from xgboost\n",
    "## -> factor analysis\n",
    "#### (what about factor analysis/pca weighted by xgboost feature importance?)\n",
    "## how about trying to fit against Con/X?% *simultaneously for 05/10/15/17(/19)\n",
    "## tsne on top X factors?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c6b5a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(range(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f60ca66",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from sklearn import manifold, datasets\n",
    "# perplexities = [5, 30, 50, 100]\n",
    "perplexity = 5\n",
    "n_components = 2\n",
    "tsne = manifold.TSNE(\n",
    "    n_components=n_components,\n",
    "    init=\"random\",\n",
    "    random_state=0,\n",
    "    perplexity=perplexity,\n",
    "    learning_rate =100,\n",
    "    n_iter=300,\n",
    ")\n",
    "Y = tsne.fit_transform(BES_decomp[[0, 1, 2, 4, 5, 6, 7, 8, ]])\n",
    "palette = {'Conservative':\"b\", 'Labour':\"r\", 'Liberal Democrat':\"orange\", 'Plaid Cymru':\"olive\",\n",
    "                  'Green':\"g\", 'Speaker':'grey', 'Scottish National Party':'y'}\n",
    "plt.figure(figsize =(8,6));\n",
    "sns.scatterplot(x=Y[:,0],y=Y[:,1],\n",
    "                hue=BES_census_data[\"Winner19\"].loc[BES_decomp.index],\n",
    "                palette=palette)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0286db08",
   "metadata": {},
   "outputs": [],
   "source": [
    "import umap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68abc87a",
   "metadata": {},
   "outputs": [],
   "source": [
    "u.min_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89789132",
   "metadata": {},
   "outputs": [],
   "source": [
    "u = umap.UMAP()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd5e89a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "reducer = umap.UMAP(n_neighbors=2, # def 15\n",
    "                    min_dist=.1,n_components=2,metric=\"euclidean\")\n",
    "embedding = reducer.fit_transform(BES_decomp[[0, 1, 2, 4, 5, 6, 7, 8, ]])\n",
    "\n",
    "plt.figure(figsize =(8,6));\n",
    "sns.scatterplot(x=embedding[:,0],y=embedding[:,1],\n",
    "                hue=BES_census_data[\"Winner19\"].loc[BES_decomp.index],\n",
    "                palette=palette)\n",
    "# Put the legend out of the figure\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ba444e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from umap.parametric_umap import ParametricUMAP\n",
    "embedder = ParametricUMAP()\n",
    "embedding = embedder.fit_transform(BES_decomp[[0, 1, 2, 4, 5, 6, 7, 8, ]])\n",
    "plt.figure(figsize =(8,6));\n",
    "sns.scatterplot(x=embedding[:,0],y=embedding[:,1],\n",
    "                hue=BES_census_data[\"Winner19\"].loc[BES_decomp.index],\n",
    "                palette=palette)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d46fc0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize =(8,6));\n",
    "# sns.scatterplot(x=BES_decomp[0],y=BES_decomp[1],\n",
    "#                 hue=BES_census_data[\"Winner19\"].loc[BES_decomp.index],\n",
    "#                 palette=palette)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "054e35c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## try PCA instead (more orth)\n",
    "## try better FA!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdbf96c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#             Y = df_simp[var_stub]\n",
    "#             X = BES_decomp.loc[:,0:n_comp]\n",
    "#             X = sm.add_constant(X)\n",
    "#             model = sm.OLS(Y,X)\n",
    "#             results = model.fit()\n",
    "#             display(results.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5ab6b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "### we want an okayish baseline for parameters\n",
    "\n",
    "### we want a mixture of combos for Coalitions and Countries\n",
    "# thresh =.1, n_components =10\n",
    "\n",
    "# Countries -> Britain/England&Wales/Scotland/England/Wales/regional combos??\n",
    "# Con/Auth-Right/Lab/Lib-Left/Lab+LD/Lab+LD+Green\n",
    "\n",
    "# ~10 -> say we want an hour/hour-and-a-half\n",
    "# 6-9 mins each"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbf7f18b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1cd323d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aff6295a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "geomask = (BES_census_data[\"Country\"]!=\"Scotland\")&(BES_census_data[\"Winner17\"]!=\"Speaker\")\n",
    "df_simp = pd.get_dummies(BES_census_data.drop(drop_cols+['Con17'],axis=1))[geomask]\n",
    "df_simp[var_stub] = BES_census_data[var_stub][geomask].values\n",
    "var_stub = \"Con17_elec\"\n",
    "\n",
    "# results_list = []\n",
    "# n_components=5\n",
    "for n_comp in [5,10]:\n",
    "    for threshold in [0,.1,.2,.3,.4,.5,.6,.7]:\n",
    "        #,0.01,.02,.05,.1\n",
    "        for repeat in range(1):\n",
    "            start = time.time()\n",
    "\n",
    "#             Treatment = var_stub\n",
    "\n",
    "#             var_list = [var_stub]\n",
    "#             var_stub_list = [var_stub,]\n",
    "#             mask = df_simp[var_stub].notnull()\n",
    "#             wt_cols = []\n",
    "#             min_features = 30\n",
    "#             colname = var_stub\n",
    "#             dependence_plots = False\n",
    "\n",
    "#             alg = get_xgboost_alg(classification_problem=False)\n",
    "#             alg.random_state = randint(0, 1000)\n",
    "#             # alg.n_estimators=1000\n",
    "#             title = var_stub\n",
    "\n",
    "#             (explainer, shap_values, train_columns, train_index, alg,output_subfolder)=\\\n",
    "#                 xgboost_run(subdir=colname,dataset=df_simp[mask].drop(wt_cols,axis=1),\n",
    "#                         var_list=var_list,var_stub_list=var_stub_list,\n",
    "#                         use_specific_weights=None,\n",
    "#                         min_features = min(df_simp.shape[1]-1,min_features),verbosity=0,\n",
    "#                         skip_bar_plot=True,dependence_plots=dependence_plots,alg=alg,eval_metric=eval_metric,                    \n",
    "#                         title = title)\n",
    "\n",
    "#             dist = pd.DataFrame(shap_values).abs().mean(axis=0).sort_values(ascending=False)\n",
    "#             print( train_columns[dist[dist>=threshold].index].shape,df_simp.shape )\n",
    "\n",
    "#             df = df_simp[train_columns[dist[dist>=threshold].index]][geomask]\n",
    "#             df = df.fillna(df.mean())\n",
    "            corrs = df_simp.drop(var_stub,axis=1).corrwith(df_simp[var_stub])\n",
    "            df = df_simp[corrs[corrs.abs()>=corr_thresh].index]\n",
    "\n",
    "            (BES_decomp, comp_labels, comp_dict) = dim_red(df, n_components=n_comp,red_type=\"Factor Analysis\",show_first_x_comps=14)\n",
    "\n",
    "            Y = df_simp[var_stub]\n",
    "            X = BES_decomp.loc[:,0:n_comp]\n",
    "            X = sm.add_constant(X)\n",
    "            model = sm.OLS(Y,X)\n",
    "            results = model.fit()\n",
    "            display(results.summary())\n",
    "\n",
    "            ind = results_df.index.max()\n",
    "            if pd.isnull(ind):\n",
    "                ind = 0\n",
    "            else:\n",
    "                ind = ind+1\n",
    "            end = time.time()\n",
    "    #         print(end - start)            \n",
    "            results_df.loc[ind,[\"duration\",\"n_components\",\"variables\",\"corr_thresh\",\"repeats\",\"results\"]] = [end-start,n_comp,\n",
    "                                                                                                           df.columns,\n",
    "                                                                                                    corr_thresh,repeat,results]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "963d4f1e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c65bd8ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a66b6d78",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36231c75",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
